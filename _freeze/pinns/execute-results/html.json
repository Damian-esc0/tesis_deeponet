{
  "hash": "12d81f11cb9ab68d44dbad8531c41c4e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\nlang: es\nexecute: \n  freeze: auto\n---\n\n# Physics Informed Neural Networks (PINNs) {#sec-pinns}\n\n::: {style=\"text-align: justify\"}\nLas Physics-Informed Neural Networks (PINNs) son un enfoque innovador que combina redes neuronales con ecuaciones diferenciales gobernantes para resolver problemas complejos de física [@blechs2021]. A diferencia de métodos tradicionales, las PINNs incorporan directamente las ecuaciones físicas en su función de pérdida mediante diferenciación automática, lo que permite minimizar simultáneamente el error en los datos y el residual de las PDEs [@karniadakis2021]. Esta característica las hace particularmente valiosas en escenarios con datos limitados, donde el conocimiento físico actúa como un regularizador efectivo. La capacidad de aproximación de las PINNs se fundamenta en el teorema de aproximación universal de las redes neuronales, adaptado para incorporar restricciones físicas a través de términos de penalización en la función de optimización [@karniadakis2021].\n\nComo ejemplo, se considera la **ecuación de Burgers para viscocidad**:\n\n$$\n\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = \\nu \\frac{\\partial^2 u}{\\partial x^2}\n$$\n\nCon una condición inicial adecuada y condiciones de contorno de Dirichlet. En la @fig-pinn_graph, la red izquierda (physics-​uninformed) representa el sustituto de la solución de EDP $u(x, t)$, mientras que la red derecha (physics-​informed) describe el residuo de EDP $\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} - \\nu \\frac{\\partial^2 u}{\\partial x^2}$. La función de pérdida incluye una pérdida supervisada de las mediciones de datos de $u$ de las condiciones iniciales y de contorno, y una pérdida no supervisada de EDP:\n\n$$\n\\mathcal{L} = w_{\\text{data}} \\mathcal{L}_{\\text{data}} + w_{\\text{PDE}} \\mathcal{L}_{\\text{PDE}}\n$$ {#eq-loss-funct}\n\ndonde:\n\n\\begin{align*}\n\\mathcal{L}_{\\text{data}} =& \\frac{1}{N_{\\text{data}}} \\sum_{i=1}^{N_{\\text{data}}} \\left( u(x_i, t_i) - u_i \\right)^2 \\\\ \\\\\n\\mathcal{L}_{\\text{PDE}} =& \\frac{1}{N_{\\text{PDE}}} \\sum_{j=1}^{N_{\\text{PDE}}} \\left( \\frac{\\partial u}{\\partial t}(x_j, t_j) + u \\frac{\\partial u}{\\partial x}(x_j, t_j) - \\nu \\frac{\\partial^2 u}{\\partial x^2}(x_j, t_j) \\right)^2\n\\end{align*}\n\nAquí, $(x_i, t_i)$ representan puntos donde se conocen valores de la solución y $(x_j, t_j)$ son puntos interiores del dominio. Los pesos $w_{\\text{data}}$ y $w_{\\text{PDE}}$ equilibran la contribución de cada término. La red se entrena minimizando $\\mathcal{L}$ usando optimizadores como Adam o L-BFGS hasta alcanzar un umbral $\\varepsilon$ [@karniadakis2021].\n\nEste enfoque permite resolver EDPs (clásicas, fraccionarias o estocásticas) sin mallas, en dominios complejos o con datos escasos y ruidosos, siendo una herramienta flexible y poderosa para la modelación científica.\n\n![**El algoritmo de una PINN.** Construya una red neuronal (NN) $u(x, t; \\theta)$ donde $\\theta$ representa el conjunto de pesos entrenables $w$ y sesgos $b$, y $\\sigma$ representa una función de activación no lineal. Especifique los datos de medición ${x_i, t_i, u_i}$ para $u$ y los puntos residuales ${x_j, t_j}$ para la EDP. Especifique la pérdida $\\mathcal{L}$ en la @eq-loss-funct sumando las pérdidas ponderadas de los datos y la EDP. Entrene la NN para encontrar los mejores parámetros $\\mathbb{\\theta^*}$ minimizando la pérdida $\\mathcal{L}$ [@karniadakis2021].](images/PINN_diagram.png){#fig-pinn_graph fig-align=\"center\" width=\"600\"}\n\n## Ejemplo de resolución de la ecuación de Burger 1D con deepxde\nDada la ecuación:\n$$\n\\dfrac{\\partial u}{\\partial t} + u\\dfrac{\\partial u}{\\partial x} = v\\dfrac{\\partial^2 u}{\\partial x^2} \\quad, x \\in[-1,1], \\ t\\in[0,1]\n$$\n\ncon la condición de frontera de Dirichlet y condición inicial:\n$$\nu(-1,t) = u(1,t) = 0, \\quad u(x,0)=-\\sin(\\pi x)\n$$\n\n::: {#871a6828 .cell execution_count=1}\n``` {.python .cell-code}\nimport deepxde as dde\nimport numpy as np\n\n\ndef gen_testdata():\n    data = np.load(\"data/Burgers.npz\")\n    t, x, exact = data[\"t\"], data[\"x\"], data[\"usol\"].T\n    xx, tt = np.meshgrid(x, t)\n    X = np.vstack((np.ravel(xx), np.ravel(tt))).T\n    y = exact.flatten()[:, None]\n    return X, y\n\n\ndef pde(x, y):\n    dy_x = dde.grad.jacobian(y, x, i=0, j=0)\n    dy_t = dde.grad.jacobian(y, x, i=0, j=1)\n    dy_xx = dde.grad.hessian(y, x, i=0, j=0)\n    return dy_t + y * dy_x - 0.01 / np.pi * dy_xx\n\n\ngeom = dde.geometry.Interval(-1, 1)\ntimedomain = dde.geometry.TimeDomain(0, 0.99)\ngeomtime = dde.geometry.GeometryXTime(geom, timedomain)\n\nbc = dde.icbc.DirichletBC(\n    geomtime,\n    lambda x: 0,\n    lambda _, on_boundary: on_boundary)\nic = dde.icbc.IC(\n    geomtime,\n    lambda x: -np.sin(np.pi * x[:, 0:1]),\n    lambda _, on_initial: on_initial\n)\n\ndata = dde.data.TimePDE(\n    geomtime, pde, [bc, ic],\n    num_domain=2540,\n    num_boundary=80,\n    num_initial=160,\n    num_test=300\n)\nnet = dde.nn.FNN([2] + [20] * 3 + [1], \"tanh\", \"Glorot normal\")\nmodel = dde.Model(data, net)\n\nmodel.compile(\"adam\", lr=1e-3)\nlosshistory, train_state = model.train(iterations=5000)\nmodel.compile(\"L-BFGS\")\nlosshistory, train_state = model.train()\n#dde.saveplot(losshistory, train_state, issave=False, isplot=True)\n```\n:::\n\n\n::: {#4b9d51ca .cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt \nfrom mpl_toolkits.mplot3d import Axes3D\n\nX, y_true = gen_testdata()\ny_pred = model.predict(X)\nf = model.predict(X, operator=pde)\n\n# Extraer componentes de X\nx_coords = X[:, 0]  # coordenadas x (espacio)\ntime = X[:, 1]      # coordenadas t (tiempo)\n\n# Crear figura con dos subgráficos 3D\nfig = plt.figure(figsize=(12, 8))\nax1 = fig.add_subplot(121, projection='3d')\nax2 = fig.add_subplot(122, projection='3d')\n\n# Calcular límites comunes para los ejes z\nz_min = min(y_true.min(), y_pred.min())\nz_max = max(y_true.max(), y_pred.max())\n\n# Gráfico 1: Valores reales\nsc1 = ax1.scatter(x_coords, time, y_true, c=y_true,\n                cmap='viridis', marker='o', vmin=z_min, vmax=z_max)\nax1.set_xlabel('Posición (x)')\nax1.set_ylabel('Tiempo (t)')\nax1.set_zlabel('u(x,t)')\nax1.set_title('Valores reales de u(x,t)')\nax1.set_zlim([z_min, z_max])\n\n# Gráfico 2: Valores predichos\nsc2 = ax2.scatter(x_coords, time, y_pred, c=y_pred,\n                cmap='viridis', marker='^', vmin=z_min, vmax=z_max)\nax2.set_xlabel('Posición (x)')\nax2.set_ylabel('Tiempo (t)')\nax2.set_zlabel('u(x,t)')\nax2.set_title('Valores predichos de u(x,t)')\nax2.set_zlim([z_min, z_max])\n\ncbar = fig.colorbar(sc1, ax=(ax1,ax2),\n                    shrink=0.9, aspect=90,\n                    pad=0.1, orientation='horizontal')\ncbar.set_label('Magnitud de u(x,t)')\n\nplt.show()\n\n\nprint(\"Mean residual:\", np.mean(np.absolute(f)))\nprint(\"L2 relative error:\", dde.metrics.l2_relative_error(y_true, y_pred))\n```\n\n::: {.cell-output .cell-output-display}\n![Resultados de la red neuronal.](pinns_files/figure-html/cell-3-output-1.png){width=934 height=554}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMean residual: 0.005167113\nL2 relative error: 0.008462455995169592\n```\n:::\n:::\n\n\n## Comparación con Redes Neuronales Tradicionales\n\nMientras que las redes neuronales tradicionales dependen exclusivamente de grandes volúmenes de datos etiquetados para su entrenamiento [@karniadakis2021], las PINNs integran el conocimiento físico como parte esencial de su arquitectura [@blechs2021]. Esta diferencia clave permite a las PINNs generar soluciones físicamente consistentes incluso con datos escasos, evitando el sobreajuste común en enfoques puramente basados en datos. Otra ventaja significativa de las PINNs es su naturaleza mesh-free, que contrasta con los métodos numéricos tradicionales como FEM o FDM que requieren discretización espacial. Sin embargo, el entrenamiento de PINNs puede ser más desafiante debido a la necesidad de optimizar múltiples objetivos simultáneamente (ajuste a datos y cumplimiento de leyes físicas) [@blechs2021; @karniadakis2021].\n\n:::\n\n",
    "supporting": [
      "pinns_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}