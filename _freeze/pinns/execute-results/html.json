{
  "hash": "a9ba825fde573a964dadf13077239667",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Physic Informed Neural Networks (PINNs)\nlang: es\nexecute: \n  freeze: auto\n---\n\n::: {style=\"text-align: justify\"}\nLas Physics-Informed Neural Networks (PINNs) son un enfoque innovador que combina redes neuronales con ecuaciones diferenciales gobernantes para resolver problemas complejos de física [@blechs2021]. A diferencia de métodos tradicionales, las PINNs incorporan directamente las ecuaciones físicas en su función de pérdida mediante diferenciación automática, lo que permite minimizar simultáneamente el error en los datos y el residual de las PDEs [@karniadakis2021]. Esta característica las hace particularmente valiosas en escenarios con datos limitados, donde el conocimiento físico actúa como un regularizador efectivo . La capacidad de aproximación de las PINNs se fundamenta en el teorema de aproximación universal de las redes neuronales, adaptado para incorporar restricciones físicas a través de términos de penalización en la función de optimización [@karniadakis2021].\n\nComo ejemplo, se considera la **ecuación de Burgers para viscocidad**:\n\n$$\n\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = \\nu \\frac{\\partial^2 u}{\\partial x^2}\n$$\n\nCon una condición inicial adecuada y condiciones de contorno de Dirichlet. En la figura @fig-pinn_graph, la red izquierda (physics-​uninformed) representa el sustituto de la solución de EDP $u(x, t)$, mientras que la red derecha (physics-​informed) describe el residuo de EDP $\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} - \\nu \\frac{\\partial^2 u}{\\partial x^2}$. La función de pérdida incluye una pérdida supervisada de las mediciones de datos de $u$ de las condiciones iniciales y de contorno, y una pérdida no supervisada de EDP:\n\n$$\n\\mathcal{L} = w_{\\text{data}} \\mathcal{L}_{\\text{data}} + w_{\\text{PDE}} \\mathcal{L}_{\\text{PDE}}\n$$ {#eq-loss-funct}\n\ndonde:\n\n\\begin{align*}\n\\mathcal{L}_{\\text{data}} =& \\frac{1}{N_{\\text{data}}} \\sum_{i=1}^{N_{\\text{data}}} \\left( u(x_i, t_i) - u_i \\right)^2 \\\\ \\\\\n\\mathcal{L}_{\\text{PDE}} =& \\frac{1}{N_{\\text{PDE}}} \\sum_{j=1}^{N_{\\text{PDE}}} \\left( \\frac{\\partial u}{\\partial t}(x_j, t_j) + u \\frac{\\partial u}{\\partial x}(x_j, t_j) - \\nu \\frac{\\partial^2 u}{\\partial x^2}(x_j, t_j) \\right)^2\n\\end{align*}\n\nAquí, $(x_i, t_i)$ representan puntos donde se conocen valores de la solución y $(x_j, t_j)$ son puntos interiores del dominio. Los pesos $w_{\\text{data}}$ y $w_{\\text{PDE}}$ equilibran la contribución de cada término. La red se entrena minimizando $\\mathcal{L}$ usando optimizadores como Adam o L-BFGS hasta alcanzar un umbral $\\varepsilon$ [@karniadakis2021].\n\nEste enfoque permite resolver EDPs (clásicas, fraccionarias o estocásticas) sin mallas, en dominios complejos o con datos escasos y ruidosos, siendo una herramienta flexible y poderosa para la modelación científica.\n\n![**El algoritmo de una PINN.** Construya una red neuronal (NN) $u(x, t; \\theta)$ donde $\\theta$ representa el conjunto de pesos entrenables $w$ y sesgos $b$, y $\\sigma$ representa una función de activación no lineal. Especifique los datos de medición ${x_i, t_i, u_i}$ para $u$ y los puntos residuales ${x_j, t_j}$ para la EDP. Especifique la pérdida $\\mathcal{L}$ en la ecuación @eq-loss-funct sumando las pérdidas ponderadas de los datos y la EDP. Entrene la NN para encontrar los mejores parámetros $\\mathbb{\\theta^*}$ minimizando la pérdida $\\mathcal{L}$ [@karniadakis2021].](images/PINN_diagram.png){#fig-pinn_graph fig-align=\"center\" width=\"600\"}\n\n## Ejemplo de resolución de la ecuación de Burger's 1D con deepxde\nDada la ecuación:\n$$\n\\dfrac{\\partial u}{\\partial t} + u\\dfrac{\\partial u}{\\partial x} = v\\dfrac{\\partial^2 u}{\\partial x^2} \\quad, x \\in[-1,1], \\ t\\in[0,1]\n$$\n\ncon la condición de frontera de Dirichlet y condición inicial:\n$$\nu(-1,t) = u(1,t) = 0, \\quad u(x,0)=-\\sin(\\pi x)\n$$\n\n::: {#669f484a .cell execution_count=1}\n``` {.python .cell-code}\nimport deepxde as dde\nimport numpy as np\n\n\ndef gen_testdata():\n    data = np.load(\"data/Burgers.npz\")\n    t, x, exact = data[\"t\"], data[\"x\"], data[\"usol\"].T\n    xx, tt = np.meshgrid(x, t)\n    X = np.vstack((np.ravel(xx), np.ravel(tt))).T\n    y = exact.flatten()[:, None]\n    return X, y\n\n\ndef pde(x, y):\n    dy_x = dde.grad.jacobian(y, x, i=0, j=0)\n    dy_t = dde.grad.jacobian(y, x, i=0, j=1)\n    dy_xx = dde.grad.hessian(y, x, i=0, j=0)\n    return dy_t + y * dy_x - 0.01 / np.pi * dy_xx\n\n\ngeom = dde.geometry.Interval(-1, 1)\ntimedomain = dde.geometry.TimeDomain(0, 0.99)\ngeomtime = dde.geometry.GeometryXTime(geom, timedomain)\n\nbc = dde.icbc.DirichletBC(geomtime, lambda x: 0, lambda _, on_boundary: on_boundary)\nic = dde.icbc.IC(\n    geomtime, lambda x: -np.sin(np.pi * x[:, 0:1]), lambda _, on_initial: on_initial\n)\n\ndata = dde.data.TimePDE(\n    geomtime, pde, [bc, ic],\n    num_domain=2540,\n    num_boundary=80,\n    num_initial=160,\n    num_test=300\n)\nnet = dde.nn.FNN([2] + [20] * 3 + [1], \"tanh\", \"Glorot normal\")\nmodel = dde.Model(data, net)\n\nmodel.compile(\"adam\", lr=1e-3)\nmodel.train(iterations=5000)\nmodel.compile(\"L-BFGS\")\nlosshistory, train_state = model.train()\ndde.saveplot(losshistory, train_state, issave=False, isplot=True)\n\nX, y_true = gen_testdata()\ny_pred = model.predict(X)\nf = model.predict(X, operator=pde)\nprint(\"Mean residual:\", np.mean(np.absolute(f)))\nprint(\"L2 relative error:\", dde.metrics.l2_relative_error(y_true, y_pred))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2025-06-25 15:38:49.026838: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2025-06-25 15:38:49.080927: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2025-06-25 15:38:49.081802: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-06-25 15:38:50.022280: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nUsing backend: tensorflow.compat.v1\nOther supported backends: tensorflow, pytorch, jax, paddle.\npaddle supports more examples now and is recommended.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nWARNING:tensorflow:From /home/damian/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\nInstructions for updating:\nnon-resource variables are not supported in the long term\nCompiling model...\nBuilding feed-forward neural network...\n'build' took 0.044881 s\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/damian/.local/lib/python3.8/site-packages/deepxde/nn/tensorflow_compat_v1/fnn.py:116: UserWarning:\n\n`tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n'compile' took 0.349554 s\n\nTraining model...\n\nStep      Train loss                        Test loss                         Test metric\n0         [9.90e-03, 5.31e-03, 5.32e-01]    [9.53e-03, 5.31e-03, 5.32e-01]    []  \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n2025-06-25 15:38:51.672960: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n1000      [3.91e-02, 4.68e-04, 5.24e-02]    [4.39e-02, 4.68e-04, 5.24e-02]    []  \n2000      [2.28e-02, 7.61e-05, 3.03e-02]    [5.67e-02, 7.61e-05, 3.03e-02]    []  \n3000      [1.05e-02, 4.22e-05, 6.66e-03]    [6.38e-02, 4.22e-05, 6.66e-03]    []  \n4000      [4.69e-03, 2.22e-05, 2.92e-03]    [3.82e-02, 2.22e-05, 2.92e-03]    []  \n5000      [3.82e-03, 1.46e-05, 2.02e-03]    [2.67e-02, 1.46e-05, 2.02e-03]    []  \n\nBest model at step 5000:\n  train loss: 5.86e-03\n  test loss: 2.88e-02\n  test metric: []\n\n'train' took 38.613541 s\n\nCompiling model...\n'compile' took 0.191548 s\n\nTraining model...\n\nStep      Train loss                        Test loss                         Test metric\n5000      [3.82e-03, 1.46e-05, 2.02e-03]    [2.67e-02, 1.46e-05, 2.02e-03]    []  \n6000      [2.79e-04, 6.99e-07, 1.37e-04]    [2.79e-04, 6.99e-07, 1.37e-04]        \n7000      [9.52e-05, 3.08e-07, 5.34e-05]    [9.52e-05, 3.08e-07, 5.34e-05]        \n8000      [3.84e-05, 1.15e-07, 1.07e-05]    [3.84e-05, 1.15e-07, 1.07e-05]        \n9000      [1.95e-05, 4.68e-08, 4.82e-06]    [1.95e-05, 4.68e-08, 4.82e-06]        \n10000     [1.15e-05, 3.38e-08, 2.46e-06]    [1.15e-05, 3.38e-08, 2.46e-06]        \nINFO:tensorflow:Optimization terminated with:\n  Message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n  Objective function value: 0.000014\n  Number of iterations: 4792\n  Number of functions evaluations: 5097\n10097     [1.14e-05, 4.10e-08, 2.28e-06]    [6.58e-03, 4.10e-08, 2.28e-06]    []  \n\nBest model at step 10097:\n  train loss: 1.37e-05\n  test loss: 6.58e-03\n  test metric: []\n\n'train' took 55.594739 s\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](pinns_files/figure-html/cell-2-output-7.png){width=581 height=429}\n:::\n\n::: {.cell-output .cell-output-display}\n![](pinns_files/figure-html/cell-2-output-8.png){width=413 height=396}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMean residual: 0.0037696785\nL2 relative error: 0.0037413991822282254\n```\n:::\n:::\n\n\n## Comparación con Redes Neuronales Tradicionales\n\nMientras que las redes neuronales tradicionales dependen exclusivamente de grandes volúmenes de datos etiquetados para su entrenamiento [@karniadakis2021], las PINNs integran el conocimiento físico como parte esencial de su arquitectura [@blechs2021]. Esta diferencia clave permite a las PINNs generar soluciones físicamente consistentes incluso con datos escasos, evitando el sobreajuste común en enfoques puramente basados en datos. Otra ventaja significativa de las PINNs es su naturaleza mesh-free, que contrasta con los métodos numéricos tradicionales como FEM o FDM que requieren discretización espacial. Sin embargo, el entrenamiento de PINNs puede ser más desafiante debido a la necesidad de optimizar múltiples objetivos simultáneamente (ajuste a datos y cumplimiento de leyes físicas) [@blechs2021; @karniadakis2021].\n\n:::\n\n",
    "supporting": [
      "pinns_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}