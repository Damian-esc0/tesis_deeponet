{
  "hash": "78e33b06ad2439f53831daff6c0ba796",
  "result": {
    "engine": "jupyter",
    "markdown": "---\nlang: es\nexecute: \n  freeze: auto\n---\n\n# Physics Informed Neural Networks (PINNs) {#sec-pinns}\n\n::: {style=\"text-align: justify\"}\nLas Physics-Informed Neural Networks (PINNs) son un enfoque innovador que combina redes neuronales con ecuaciones diferenciales gobernantes para resolver problemas complejos de física [@blechs2021]. A diferencia de métodos tradicionales, las PINNs incorporan directamente las ecuaciones físicas en su función de pérdida mediante diferenciación automática, lo que permite minimizar simultáneamente el error en los datos y el residual de las PDEs [@karniadakis2021]. Esta característica las hace particularmente valiosas en escenarios con datos limitados, donde el conocimiento físico actúa como un regularizador efectivo. La capacidad de aproximación de las PINNs se fundamenta en el teorema de aproximación universal de las redes neuronales, adaptado para incorporar restricciones físicas a través de términos de penalización en la función de optimización [@karniadakis2021].\n\nComo ejemplo, se considera la **ecuación de Burgers para viscocidad**:\n\n$$\n\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = \\nu \\frac{\\partial^2 u}{\\partial x^2}\n$$\n\nCon una condición inicial adecuada y condiciones de contorno de Dirichlet. En la @fig-pinn_graph, la red izquierda (physics-​uninformed) representa el sustituto de la solución de EDP $u(x, t)$, mientras que la red derecha (physics-​informed) describe el residuo de EDP $\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} - \\nu \\frac{\\partial^2 u}{\\partial x^2} = 0$. La función de pérdida incluye una pérdida supervisada de las mediciones de datos de $u$ de las condiciones iniciales y de contorno, y una pérdida no supervisada de EDP:\n\n$$\n\\mathcal{L} = w_{\\text{data}} \\mathcal{L}_{\\text{data}} + w_{\\text{PDE}} \\mathcal{L}_{\\text{PDE}}\n$$ {#eq-loss-funct}\n\ndonde:\n\n\\begin{align*}\n\\mathcal{L}_{\\text{data}} =& \\frac{1}{N_{\\text{data}}} \\sum_{i=1}^{N_{\\text{data}}} \\left( u(x_i, t_i) - u_i \\right)^2 \\\\ \\\\\n\\mathcal{L}_{\\text{PDE}} =& \\frac{1}{N_{\\text{PDE}}} \\sum_{j=1}^{N_{\\text{PDE}}} \\left( \\frac{\\partial u}{\\partial t}(x_j, t_j) + u \\frac{\\partial u}{\\partial x}(x_j, t_j) - \\nu \\frac{\\partial^2 u}{\\partial x^2}(x_j, t_j) \\right)^2\n\\end{align*}\n\nAquí, $(x_i, t_i)$ representan puntos donde se conocen valores de la solución y $(x_j, t_j)$ son puntos interiores del dominio. Los pesos $w_{\\text{data}}$ y $w_{\\text{PDE}}$ equilibran la contribución de cada término. La red se entrena minimizando $\\mathcal{L}$ usando optimizadores como Adam o L-BFGS hasta alcanzar un umbral $\\varepsilon$ [@karniadakis2021].\n\nEste enfoque permite resolver EDPs (clásicas, fraccionarias o estocásticas) sin mallas, en dominios complejos o con datos escasos y ruidosos, siendo una herramienta flexible y poderosa para la modelación científica.\n\n![**El algoritmo de una PINN.** Construya una red neuronal (NN) $u(x, t; \\theta)$ donde $\\theta$ representa el conjunto de pesos entrenables $w$ y sesgos $b$, y $\\sigma$ representa una función de activación no lineal. Especifique los datos de medición ${x_i, t_i, u_i}$ para $u$ y los puntos residuales ${x_j, t_j}$ para la EDP. Especifique la pérdida $\\mathcal{L}$ en la @eq-loss-funct sumando las pérdidas ponderadas de los datos y la EDP. Entrene la NN para encontrar los mejores parámetros $\\mathbb{\\theta^*}$ minimizando la pérdida $\\mathcal{L}$ [@karniadakis2021].](images/PINN_diagram.png){#fig-pinn_graph fig-align=\"center\" width=\"600\" .lightbox}\n\n## Algoritmos de optimización {#sec-optimizadores}\nUn algoritmo de optimización busca minimizar o maximizar una función objetivo ajustando sus parámetros de manera iterativa. Son esenciales en el entrenamiento de redes neuronales y otros modelos de aprendizaje automático [@kingma2014adam].\n\n### ADAM\n**Adaptive Moment Estimation** (*ADAM*) combina estimaciones de primer y segundo momento del gradiente para adaptar las tasas de aprendizaje por parámetro. Utiliza promedios móviles exponenciales de gradientes y gradientes al cuadrado, corregidos por bias, lo que lo hace eficiente en problemas con gradientes ruidosos o dispersos. Es robusto y requiere poco ajuste hiperparamétrico [@kingma2014adam].\n\n### L-BFGS\n**Limited-memory BFGS** (*L-BFGS*) es un método quasi-Newton que aproxima la inversa del Hessiano usando un historial limitado de gradientes y actualizaciones de parámetros. Evita el costo computacional de almacenar matrices completas, lo que lo hace viable para problemas de alta dimensionalidad. Es especialmente útil en optimización batch o con gradientes estables [@Goldfarb2016lbfgs].\n\n## Deepxde\nDeepXDE es una biblioteca en Python de aprendizaje profundo diseñada para resolver ecuaciones diferenciales, incluyendo ecuaciones diferenciales parciales (PDEs), ecuaciones integro-diferenciales (IDEs) y ecuaciones diferenciales estocásticas (SDEs), utilizando redes neuronales informadas por la física (PINNs). Combina técnicas de aprendizaje automático con principios físicos al incorporar las ecuaciones diferenciales directamente en la función de pérdida de la red neuronal, aprovechando la diferenciación automática para calcular derivadas de manera precisa y eficiente [@deepxde].\n\n## Ejemplo de resolución de la ecuación de Burger 1D con deepxde\nDada la ecuación:\n$$\n\\dfrac{\\partial u}{\\partial t} + u\\dfrac{\\partial u}{\\partial x} = v\\dfrac{\\partial^2 u}{\\partial x^2}, \\quad x \\in[-1,1], \\ t\\in[0,1],\n$$\n\ncon la condición de frontera de Dirichlet y condición inicial:\n$$\nu(-1,t) = u(1,t) = 0, \\quad u(x,0)=-\\sin(\\pi x).\n$$\n\n::: {#5b4897bd .cell execution_count=1}\n``` {.python .cell-code}\nimport deepxde as dde\nimport numpy as np\n\n# Definir una función para cargar los datos\ndef gen_testdata():\n    data = np.load(\"data/Burgers.npz\")\n    t, x, exact = data[\"t\"], data[\"x\"], data[\"usol\"].T\n    xx, tt = np.meshgrid(x, t)\n    X = np.vstack((np.ravel(xx), np.ravel(tt))).T\n    y = exact.flatten()[:, None]\n    return X, y\n\n# Definir la PDE\ndef pde(x, y):\n    dy_x = dde.grad.jacobian(y, x, i=0, j=0)\n    dy_t = dde.grad.jacobian(y, x, i=0, j=1)\n    dy_xx = dde.grad.hessian(y, x, i=0, j=0)\n    return dy_t + y * dy_x - 0.01 / np.pi * dy_xx\n\n# Definir los dominios espacial, temporal y juntarlos\ngeom = dde.geometry.Interval(-1, 1)\ntimedomain = dde.geometry.TimeDomain(0, 0.99)\ngeomtime = dde.geometry.GeometryXTime(geom, timedomain)\n\n# Definir la condición de frontera\nbc = dde.icbc.DirichletBC(\n    geomtime,\n    lambda x: 0,\n    lambda _, on_boundary: on_boundary)\n\n# Definir la condición inicial\nic = dde.icbc.IC(\n    geomtime,\n    lambda x: -np.sin(np.pi * x[:, 0:1]),\n    lambda _, on_initial: on_initial\n)\n\n# Definir la cantidad de puntos en el dominio\ndata = dde.data.TimePDE(\n    geomtime, pde, [bc, ic],\n    num_domain=2540,\n    num_boundary=80,\n    num_initial=160,\n    num_test=300\n)\n\n# Definir la arquitectura de la red, así como\n# su función de activación y el inicializador\nnet = dde.nn.FNN([2] + [20] * 3 + [1], \"tanh\", \"Glorot normal\")\nmodel = dde.Model(data, net)\n\n# Compilar el modelo y entrenarlo\nmodel.compile(\"adam\", lr=1e-3)\nlosshistory, train_state = model.train(iterations=5000)\nmodel.compile(\"L-BFGS\")\nlosshistory, train_state = model.train()\n#dde.saveplot(losshistory, train_state, issave=False, isplot=True)\n```\n:::\n\n\n::: {#3bf73ee9 .cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt \nfrom mpl_toolkits.mplot3d import Axes3D\n\nX, y_true = gen_testdata()\ny_pred = model.predict(X)\nf = model.predict(X, operator=pde)\n\n# Extraer componentes de X\nx_coords = X[:, 0]  # coordenadas x (espacio)\ntime = X[:, 1]      # coordenadas t (tiempo)\n\n# Crear figura con dos subgráficos 3D\nfig = plt.figure(figsize=(12, 9))\nax1 = fig.add_subplot(121, projection='3d')\nax2 = fig.add_subplot(122, projection='3d')\n\n# Calcular límites comunes para los ejes z\nz_min = min(y_true.min(), y_pred.min())\nz_max = max(y_true.max(), y_pred.max())\n\n# Gráfico 1: Valores reales\nsc1 = ax1.scatter(x_coords, time, y_true, c=y_true,\n                cmap='viridis', marker='o', vmin=z_min, vmax=z_max)\nax1.set_xlabel('Posición (x)')\nax1.set_ylabel('Tiempo (t)')\nax1.set_zlabel('u(x,t)')\nax1.set_title('Valores reales de u(x,t)')\nax1.set_zlim([z_min, z_max])\n\n# Gráfico 2: Valores predichos\nsc2 = ax2.scatter(x_coords, time, y_pred, c=y_pred,\n                cmap='viridis', marker='^', vmin=z_min, vmax=z_max)\nax2.set_xlabel('Posición (x)')\nax2.set_ylabel('Tiempo (t)')\nax2.set_zlabel('u(x,t)')\nax2.set_title('Valores predichos de u(x,t)')\nax2.set_zlim([z_min, z_max])\n\ncbar = fig.colorbar(sc1, ax=(ax1,ax2),\n                    shrink=0.9, aspect=90,\n                    pad=0.1, orientation='horizontal')\ncbar.set_label('Magnitud de u(x,t)')\n\nplt.show()\n\nprint(\"Error relativo L2:\", dde.metrics.l2_relative_error(y_true, y_pred))\n```\n\n::: {.cell-output .cell-output-display}\n![Comparación entre solución real y predicción de la red neuronal para la ecuación de Burger 1D. Debido a su naturaleza unidimensional, es posible plasmar en un eje al tiempo (t) y representar a la función a lo largo de éste como una serie de *fotos* para un instante $t$ dado.](pinns_files/figure-html/cell-3-output-1.png){width=934 height=561}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nError relativo L2: 0.006932442610063683\n```\n:::\n:::\n\n\n## Comparación con Redes Neuronales Tradicionales\n\nMientras que las redes neuronales tradicionales dependen exclusivamente de grandes volúmenes de datos etiquetados para su entrenamiento [@karniadakis2021], las PINNs integran el conocimiento físico como parte esencial de su arquitectura [@blechs2021]. Esta diferencia clave permite a las PINNs generar soluciones físicamente consistentes incluso con datos escasos, evitando el sobreajuste común en enfoques puramente basados en datos. Otra ventaja significativa de las PINNs es su naturaleza mesh-free, que contrasta con los métodos numéricos tradicionales como FEM (*Finite Element Method*) o FDM (*Finite Difference Method*) que requieren discretización espacial. Sin embargo, el entrenamiento de PINNs puede ser más desafiante debido a la necesidad de optimizar múltiples objetivos simultáneamente (ajuste a datos y cumplimiento de leyes físicas) [@blechs2021; @karniadakis2021].\n\n:::\n\n",
    "supporting": [
      "pinns_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}