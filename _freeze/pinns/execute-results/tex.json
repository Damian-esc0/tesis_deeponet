{
  "hash": "a9ba825fde573a964dadf13077239667",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Physic Informed Neural Networks (PINNs)\nlang: es\nexecute: \n  freeze: auto\n---\n\n::: {style=\"text-align: justify\"}\nLas Physics-Informed Neural Networks (PINNs) son un enfoque innovador que combina redes neuronales con ecuaciones diferenciales gobernantes para resolver problemas complejos de física [@blechs2021]. A diferencia de métodos tradicionales, las PINNs incorporan directamente las ecuaciones físicas en su función de pérdida mediante diferenciación automática, lo que permite minimizar simultáneamente el error en los datos y el residual de las PDEs [@karniadakis2021]. Esta característica las hace particularmente valiosas en escenarios con datos limitados, donde el conocimiento físico actúa como un regularizador efectivo . La capacidad de aproximación de las PINNs se fundamenta en el teorema de aproximación universal de las redes neuronales, adaptado para incorporar restricciones físicas a través de términos de penalización en la función de optimización [@karniadakis2021].\n\nComo ejemplo, se considera la **ecuación de Burgers para viscocidad**:\n\n$$\n\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = \\nu \\frac{\\partial^2 u}{\\partial x^2}\n$$\n\nCon una condición inicial adecuada y condiciones de contorno de Dirichlet. En la figura @fig-pinn_graph, la red izquierda (physics-​uninformed) representa el sustituto de la solución de EDP $u(x, t)$, mientras que la red derecha (physics-​informed) describe el residuo de EDP $\\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} - \\nu \\frac{\\partial^2 u}{\\partial x^2}$. La función de pérdida incluye una pérdida supervisada de las mediciones de datos de $u$ de las condiciones iniciales y de contorno, y una pérdida no supervisada de EDP:\n\n$$\n\\mathcal{L} = w_{\\text{data}} \\mathcal{L}_{\\text{data}} + w_{\\text{PDE}} \\mathcal{L}_{\\text{PDE}}\n$$ {#eq-loss-funct}\n\ndonde:\n\n\\begin{align*}\n\\mathcal{L}_{\\text{data}} =& \\frac{1}{N_{\\text{data}}} \\sum_{i=1}^{N_{\\text{data}}} \\left( u(x_i, t_i) - u_i \\right)^2 \\\\ \\\\\n\\mathcal{L}_{\\text{PDE}} =& \\frac{1}{N_{\\text{PDE}}} \\sum_{j=1}^{N_{\\text{PDE}}} \\left( \\frac{\\partial u}{\\partial t}(x_j, t_j) + u \\frac{\\partial u}{\\partial x}(x_j, t_j) - \\nu \\frac{\\partial^2 u}{\\partial x^2}(x_j, t_j) \\right)^2\n\\end{align*}\n\nAquí, $(x_i, t_i)$ representan puntos donde se conocen valores de la solución y $(x_j, t_j)$ son puntos interiores del dominio. Los pesos $w_{\\text{data}}$ y $w_{\\text{PDE}}$ equilibran la contribución de cada término. La red se entrena minimizando $\\mathcal{L}$ usando optimizadores como Adam o L-BFGS hasta alcanzar un umbral $\\varepsilon$ [@karniadakis2021].\n\nEste enfoque permite resolver EDPs (clásicas, fraccionarias o estocásticas) sin mallas, en dominios complejos o con datos escasos y ruidosos, siendo una herramienta flexible y poderosa para la modelación científica.\n\n![**El algoritmo de una PINN.** Construya una red neuronal (NN) $u(x, t; \\theta)$ donde $\\theta$ representa el conjunto de pesos entrenables $w$ y sesgos $b$, y $\\sigma$ representa una función de activación no lineal. Especifique los datos de medición ${x_i, t_i, u_i}$ para $u$ y los puntos residuales ${x_j, t_j}$ para la EDP. Especifique la pérdida $\\mathcal{L}$ en la ecuación @eq-loss-funct sumando las pérdidas ponderadas de los datos y la EDP. Entrene la NN para encontrar los mejores parámetros $\\mathbb{\\theta^*}$ minimizando la pérdida $\\mathcal{L}$ [@karniadakis2021].](images/PINN_diagram.png){#fig-pinn_graph fig-align=\"center\" width=\"600\"}\n\n## Ejemplo de resolución de la ecuación de Burger 1D con deepxde\nDada la ecuación:\n$$\n\\dfrac{\\partial u}{\\partial t} + u\\dfrac{\\partial u}{\\partial x} = v\\dfrac{\\partial^2 u}{\\partial x^2} \\quad, x \\in[-1,1], \\ t\\in[0,1]\n$$\n\ncon la condición de frontera de Dirichlet y condición inicial:\n$$\nu(-1,t) = u(1,t) = 0, \\quad u(x,0)=-\\sin(\\pi x)\n$$\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport deepxde as dde\nimport numpy as np\n\n\ndef gen_testdata():\n    data = np.load(\"data/Burgers.npz\")\n    t, x, exact = data[\"t\"], data[\"x\"], data[\"usol\"].T\n    xx, tt = np.meshgrid(x, t)\n    X = np.vstack((np.ravel(xx), np.ravel(tt))).T\n    y = exact.flatten()[:, None]\n    return X, y\n\n\ndef pde(x, y):\n    dy_x = dde.grad.jacobian(y, x, i=0, j=0)\n    dy_t = dde.grad.jacobian(y, x, i=0, j=1)\n    dy_xx = dde.grad.hessian(y, x, i=0, j=0)\n    return dy_t + y * dy_x - 0.01 / np.pi * dy_xx\n\n\ngeom = dde.geometry.Interval(-1, 1)\ntimedomain = dde.geometry.TimeDomain(0, 0.99)\ngeomtime = dde.geometry.GeometryXTime(geom, timedomain)\n\nbc = dde.icbc.DirichletBC(geomtime, lambda x: 0, lambda _, on_boundary: on_boundary)\nic = dde.icbc.IC(\n    geomtime, lambda x: -np.sin(np.pi * x[:, 0:1]), lambda _, on_initial: on_initial\n)\n\ndata = dde.data.TimePDE(\n    geomtime, pde, [bc, ic],\n    num_domain=2540,\n    num_boundary=80,\n    num_initial=160,\n    num_test=300\n)\nnet = dde.nn.FNN([2] + [20] * 3 + [1], \"tanh\", \"Glorot normal\")\nmodel = dde.Model(data, net)\n\nmodel.compile(\"adam\", lr=1e-3)\nmodel.train(iterations=5000)\nmodel.compile(\"L-BFGS\")\nlosshistory, train_state = model.train()\ndde.saveplot(losshistory, train_state, issave=False, isplot=True)\n\nX, y_true = gen_testdata()\ny_pred = model.predict(X)\nf = model.predict(X, operator=pde)\nprint(\"Mean residual:\", np.mean(np.absolute(f)))\nprint(\"L2 relative error:\", dde.metrics.l2_relative_error(y_true, y_pred))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n2025-06-25 15:55:47.861593: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2025-06-25 15:55:47.908201: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2025-06-25 15:55:47.909072: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-06-25 15:55:48.764102: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nUsing backend: tensorflow.compat.v1\nOther supported backends: tensorflow, pytorch, jax, paddle.\npaddle supports more examples now and is recommended.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nWARNING:tensorflow:From /home/damian/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\nInstructions for updating:\nnon-resource variables are not supported in the long term\nCompiling model...\nBuilding feed-forward neural network...\n'build' took 0.045695 s\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/damian/.local/lib/python3.8/site-packages/deepxde/nn/tensorflow_compat_v1/fnn.py:116: UserWarning:\n\n`tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n'compile' took 0.365102 s\n\nTraining model...\n\nStep      Train loss                        Test loss                         Test metric\n0         [2.03e-01, 5.38e-02, 5.46e-01]    [1.97e-01, 5.38e-02, 5.46e-01]    []  \n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n2025-06-25 15:55:50.306423: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n1000      [4.10e-02, 1.67e-03, 6.36e-02]    [4.39e-02, 1.67e-03, 6.36e-02]    []  \n2000      [3.14e-02, 1.44e-04, 4.50e-02]    [5.51e-02, 1.44e-04, 4.50e-02]    []  \n3000      [2.06e-02, 7.27e-05, 3.10e-02]    [1.08e-01, 7.27e-05, 3.10e-02]    []  \n4000      [6.45e-03, 8.74e-05, 5.17e-03]    [1.69e-02, 8.74e-05, 5.17e-03]    []  \n5000      [3.00e-03, 2.28e-05, 2.00e-03]    [5.79e-03, 2.28e-05, 2.00e-03]    []  \n\nBest model at step 5000:\n  train loss: 5.02e-03\n  test loss: 7.81e-03\n  test metric: []\n\n'train' took 36.179496 s\n\nCompiling model...\n'compile' took 0.178539 s\n\nTraining model...\n\nStep      Train loss                        Test loss                         Test metric\n5000      [3.00e-03, 2.28e-05, 2.00e-03]    [5.79e-03, 2.28e-05, 2.00e-03]    []  \n6000      [2.76e-04, 1.85e-06, 2.06e-04]    [2.76e-04, 1.85e-06, 2.06e-04]        \n7000      [1.42e-04, 3.94e-07, 9.28e-05]    [1.42e-04, 3.94e-07, 9.28e-05]        \n8000      [8.45e-05, 4.89e-07, 3.93e-05]    [8.45e-05, 4.89e-07, 3.93e-05]        \n9000      [4.75e-05, 2.75e-07, 1.35e-05]    [4.75e-05, 2.75e-07, 1.35e-05]        \n10000     [2.71e-05, 1.15e-07, 6.31e-06]    [2.71e-05, 1.15e-07, 6.31e-06]        \n11000     [1.70e-05, 1.59e-07, 2.59e-06]    [1.70e-05, 1.59e-07, 2.59e-06]        \n12000     [1.13e-05, 7.90e-08, 1.77e-06]    [1.13e-05, 7.90e-08, 1.77e-06]        \n13000     [9.05e-06, 5.33e-08, 1.09e-06]    [9.05e-06, 5.33e-08, 1.09e-06]        \nINFO:tensorflow:Optimization terminated with:\n  Message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n  Objective function value: 0.000009\n  Number of iterations: 7837\n  Number of functions evaluations: 8507\n13507     [7.79e-06, 5.75e-08, 7.62e-07]    [3.06e-02, 5.75e-08, 7.62e-07]    []  \n\nBest model at step 13507:\n  train loss: 8.61e-06\n  test loss: 3.06e-02\n  test metric: []\n\n'train' took 88.146945 s\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](pinns_files/figure-pdf/cell-2-output-7.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-display}\n![](pinns_files/figure-pdf/cell-2-output-8.pdf){fig-pos='H'}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nMean residual: 0.003893385\nL2 relative error: 0.017751091603930687\n```\n:::\n:::\n\n\n## Comparación con Redes Neuronales Tradicionales\n\nMientras que las redes neuronales tradicionales dependen exclusivamente de grandes volúmenes de datos etiquetados para su entrenamiento [@karniadakis2021], las PINNs integran el conocimiento físico como parte esencial de su arquitectura [@blechs2021]. Esta diferencia clave permite a las PINNs generar soluciones físicamente consistentes incluso con datos escasos, evitando el sobreajuste común en enfoques puramente basados en datos. Otra ventaja significativa de las PINNs es su naturaleza mesh-free, que contrasta con los métodos numéricos tradicionales como FEM o FDM que requieren discretización espacial. Sin embargo, el entrenamiento de PINNs puede ser más desafiante debido a la necesidad de optimizar múltiples objetivos simultáneamente (ajuste a datos y cumplimiento de leyes físicas) [@blechs2021; @karniadakis2021].\n\n:::\n\n",
    "supporting": [
      "pinns_files/figure-pdf"
    ],
    "filters": []
  }
}