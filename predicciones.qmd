---
title: Predicciones del modelo
lang: es
execute: 
  freeze: auto
---
::: {style="text-align: justify"}
Conforme se ha referido previamente, se creó el modelo utilizando Deepxde como base. Resulta relevante descatacar que se empleó la versión 1.10.1 de dicha librería. A continuación se presenta el código fuente de la red neuronal
```{python}
import deepxde as dde
import numpy as np
import tensorflow as tf

# ------------------------------------------------------------------------------
# Constants and Parameters
# ------------------------------------------------------------------------------

# Backend and seed
dde.backend.set_default_backend("tensorflow.compat.v1")
dde.config.set_random_seed(123)

# Physical parameters
p = 1050
c = 3639
keff = 5
tf = 1800
L0 = 0.05
cb = 3825
Q = 0
TM = 45
Ta = 37
alpha = p * c / keff

# Dimensionless coefficients
a1 = tf / (alpha * L0**2)
a2 = tf * cb / (p * c)
a3 = (tf * Q) / (p * c * (TM - Ta))

# Domain boundaries
x_initial, x_boundary = 0.0, 1.0
y_initial, y_boundary = 0.0, 1.0
t_initial, t_final = 0.0, 1.0

# Dataset configuration
pts_dom = 10
pts_bc = 20
pts_ic = 60
num_test = 25

# Sensor grid and function space
num_sensors = 4
size_cov_matrix = 40

# Network architecture
width_net = 20
len_net = 3
AF = "elu"
k_initializer = "Glorot normal"

# Training parameters
num_iterations = 1000
learning_rate = 2e-3
decay_rate = 0.05
decay_steps = 1000

# ------------------------------------------------------------------------------
# Geometry and Time Domain
# ------------------------------------------------------------------------------

spatial_domain = dde.geometry.Rectangle([x_initial, y_initial],
                                        [x_boundary, y_boundary])
time_domain = dde.geometry.TimeDomain(t_initial, t_final)
geomtime = dde.geometry.GeometryXTime(spatial_domain, time_domain)

# ------------------------------------------------------------------------------
# PDE and Conditions
# ------------------------------------------------------------------------------

def initial_condition(X):
    return 0

def heat_equation(func, u, coords):
    u_t = dde.grad.jacobian(u, func, i=0, j=2)
    u_xx = dde.grad.hessian(u, func, i=0, j=0)
    u_yy = dde.grad.hessian(u, func, i=1, j=1)
    return a1 * u_t - (u_xx + u_yy) + a2 * u

def zero_value(X):
    return 0

def time_value(X):
    return X[:, 2]

def is_on_vertex(x):
    vertices = np.array([[x_initial, y_initial],
                         [x_boundary, y_initial],
                         [x_initial, y_boundary],
                         [x_boundary, y_boundary]])
    return any(np.allclose(x, v) for v in vertices)

def is_initial(X, on_initial):
    return on_initial and np.isclose(X[2], t_initial)

def left_boundary(X, on_boundary):
    spatial = X[0:2]
    t = X[2]
    return (
        on_boundary 
        and np.isclose(spatial[0], x_initial) 
        and not np.isclose(t, t_initial) 
        and not is_on_vertex(spatial)
    )

def right_boundary(X, on_boundary):
    spatial = X[0:2]
    t = X[2]
    return (
        on_boundary 
        and np.isclose(spatial[0], x_boundary) 
        and not np.isclose(t, t_initial) 
        and not is_on_vertex(spatial)
    )

def up_low_boundary(X, on_boundary):
    spatial = X[0:2]
    t = X[2]
    return (on_boundary 
    and (np.isclose(spatial[1], y_initial) 
    or np.isclose(spatial[1], y_boundary)) 
    and not np.isclose(t, t_initial) 
    and not is_on_vertex(spatial)
    )

# Initial and boundary conditions
ic = dde.icbc.IC(geomtime, initial_condition, is_initial)
left_bc = dde.icbc.DirichletBC(geomtime, 
                                zero_value, left_boundary)
right_bc = dde.icbc.NeumannBC(geomtime,
                                time_value, right_boundary)
up_low_bc = dde.icbc.NeumannBC(geomtime, 
                                zero_value, up_low_boundary)

# ------------------------------------------------------------------------------
# Dataset Construction
# ------------------------------------------------------------------------------

pde_data = dde.data.TimePDE(
    geomtime,
    heat_equation,
    [ic, left_bc, right_bc, up_low_bc],
    num_domain=pts_dom,
    num_boundary=pts_bc,
    num_initial=pts_ic 
)

# ------------------------------------------------------------------------------
# Sensor Points and Function Space
# ------------------------------------------------------------------------------

side = np.linspace(x_initial, x_boundary, num_sensors + 1)
x, y = np.meshgrid(side, side, indexing='xy')
sensor_pts = np.stack([x.ravel(), y.ravel()], axis=1)

fs = dde.data.function_spaces.GRF2D(N=size_cov_matrix, 
                                    interp="linear")

data = dde.data.PDEOperatorCartesianProd(
    pde_data,
    fs,
    sensor_pts,
    num_function=(num_sensors + 1)**2,
    function_variables=[0, 1],
    num_test=num_test
)

# ------------------------------------------------------------------------------
# Network Definition
# ------------------------------------------------------------------------------

branch_layers = [(num_sensors + 1)**2] + len_net * [width_net]
trunk_layers = [3] + len_net * [width_net]

net = dde.nn.DeepONetCartesianProd(
    branch_layers,
    trunk_layers,
    activation=AF,
    kernel_initializer=k_initializer
)

# ------------------------------------------------------------------------------
# Model Compilation and Training
# ------------------------------------------------------------------------------

model = dde.Model(data, net)
model.compile("adam", lr=learning_rate, decay=("inverse time", decay_steps, decay_rate))
losshistory, train_state = model.train(iterations=num_iterations)

# Fine-tuning with LBFGS optimizer
model.compile("L-BFGS")
losshistory, train_state = model.train()
```

El historial de perdida para el conjunto de entrenamiento es el siguiente:
```{python}
#| output: false

import plotly.graph_objects as go

# Nombres de las componentes del loss
loss_labels = [
    "PDE residual loss",
    "Initial‐condition loss",
    "Left‐boundary (Dirichlet) loss",
    "Right‐boundary (Neumann) loss",
    "Top/Bottom‐boundary (Neumann) loss"
]

# Extraer pasos y pérdida de entrenamiento
steps = losshistory.steps
train_loss = np.array(losshistory.loss_train)

# Crear figura
fig_train = go.Figure()

for i in range(train_loss.shape[1]):
    fig_train.add_trace(go.Scatter(
        x=steps,
        y=train_loss[:, i],
        mode='lines',
        name=loss_labels[i]
    ))

fig_train.update_layout(
    title="Training Loss history",
    xaxis=dict(title="Iteration", tickformat=".1e"),
    yaxis=dict(title="Loss", type="log", tickformat=".1e"),
    template="plotly_white",
    legend=dict(x=0.99, y=0.99),
    font=dict(size=14)
)
fig_train.show()
```

::: {.content-visible when-format="html"}
```{python}
#| label: fig-training_loss
#| fig-cap: "Historial de pérdida en el entrenamiento de la red neuronal."
#| echo: false

fig_train.show()
```
:::

::: {.content-visible when-format="pdf"}
```{python}
#| include: false
import numpy as np
import os
# Asegurar que la carpeta 'images' exista
os.makedirs("images", exist_ok=True)
# Exportar imagen para PDF
fig_train.write_image("images/fig-training_loss.png", width=800, height=600)
```

![Gráfica de la perdida en el entrenamiento.](images/fig-training_loss.png){#fig-loss_training fig-align="center"}
:::



El historial de perdida para el conjunto de prueba es el siguiente:
```{python}
#| output: false

import plotly.graph_objects as go

# Nombres de las componentes del loss
loss_labels = [
    "PDE residual loss",
    "Initial‐condition loss",
    "Left‐boundary (Dirichlet) loss",
    "Right‐boundary (Neumann) loss",
    "Top/Bottom‐boundary (Neumann) loss"
]

# Extraer pasos y pérdida de entrenamiento
steps = losshistory.steps
test_loss = np.array(losshistory.loss_test)

# Crear figura
fig_test = go.Figure()

for i in range(test_loss.shape[1]):
    fig_test.add_trace(go.Scatter(
        x=steps,
        y=test_loss[:, i],
        mode='lines',
        name=loss_labels[i]
    ))

fig_test.update_layout(
    title="Test Loss history",
    xaxis=dict(title="Iteration", tickformat=".1e"),
    yaxis=dict(title="Loss", type="log", tickformat=".1e"),
    template="plotly_white",
    legend=dict(x=0.99, y=0.99),
    font=dict(size=14)
)
```

::: {.content-visible when-format="html"}
```{python}
#| label: fig-test_loss
#| fig-cap: "Historial de pérdida en el conjunto de prueba de la red neuronal."
#| echo: false

fig_test.show()
```

:::

::: {.content-visible when-format="pdf"}
```{python}
#| include: false
import numpy as np
import os
# Asegurar que la carpeta 'images' exista
os.makedirs("images", exist_ok=True)
# Exportar imagen para PDF
fig_test.write_image("images/fig-test_loss.png", width=800, height=600)
```

![Gráfica de la perdida en el conjunto de prueba.](images/fig-test_loss.png){#fig-loss_test fig-align="center"}
:::

Para guardar los datos y porteriormente graficar, se uso el modelo para predecir los valores en el cuadrado de $[0,1]\times[0,1]$ dividiendo sus lados en 26 segementos equiespaciados a los tiempos $t=[0.0,0.25,0.50,0.75,1.0]$.

```{python}
import pandas as pd
# Lista de tiempos
times = [0.0, 0.25, 0.5, 0.75, 1.0]

# Crear la malla (x, y)
num_points = 26
x = np.linspace(0, 1, num_points)
y = np.linspace(0, 1, num_points)
X, Y = np.meshgrid(x, y)

# Lista para almacenar resultados
results = []

for t_val in times:
    # Crear entrada trunk: (num_points^2, 3)
    points = np.vstack((X.flatten(), Y.flatten(), t_val * np.ones_like(X.flatten()))).T

    # Crear entrada branch: condición inicial constante cero
    branch_input = np.zeros((1, sensor_pts.shape[0]))

    # Predecir
    predicted = model.predict((branch_input, points)).flatten()

    # Agregar los datos al resultado
    for xi, yi, thetai in zip(points[:, 0], points[:, 1], predicted):
        results.append([t_val, xi, yi, thetai])

# Crear el DataFrame
df = pd.DataFrame(results, columns=["time", "X", "Y", "Theta"])

# Obtener la ruta del script actual y guardar el archivo CSV
ruta = r"data/model_DoN.csv"
df.to_csv(ruta, index=False)

```

::: {.content-visible when-format="html"}
Se presentan los datos de las predicciones
```{python}
#| label: tbl-data-model
#| tbl-cap: "Predicciones de la red neuronal."
#| warning: false
#| message: false

import pandas as pd
from itables import show, options

# Leer el archivo CSV
datos = pd.read_csv("data/model_DoN.csv")

# Opcional: Aumentar el límite de bytes permitidos
options.maxBytes = 0  # 0 significa "sin límite"

# Mostrar la tabla interactiva
show(datos, paging=True, searching=True, ordering=True, buttons=["copy", "csv", "excel"])
```
:::

A continuación, los valores predichos por la red neuronal a tiempos antes mencionados. 
```{python}
#| label: fig-my_results
#| fig-cap: "Predicciones de la red neuronal a distintos tiempos."

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.gridspec as gridspec

# Cargar el DataFrame desde CSV
#df = pd.read_csv("predicciones_biocalor.csv")

# Lista de tiempos
#times = [0.0, 0.25, 0.5, 0.75, 1.0]

# Crear una figura y layout con GridSpec
ncols = len(times)
fig = plt.figure(figsize=((5 * ncols) + 1, 6))
gs = gridspec.GridSpec(2, ncols, height_ratios=[10, 1], hspace=0.3)

# Lista para almacenar los objetos surface
surf_list = []

# Asumimos que el grid es regular, así que podemos inferir la forma
num_points = int(np.sqrt(df[df["time"] == times[0]].shape[0]))

# Reordenar para graficar
for i, t_val in enumerate(times):
    # Filtrar por tiempo actual
    df_t = df[df["time"] == t_val]

    # Obtener los valores de X, Y, Theta
    X_vals = df_t["X"].values.reshape((num_points, num_points))
    Y_vals = df_t["Y"].values.reshape((num_points, num_points))
    Z_vals = df_t["Theta"].values.reshape((num_points, num_points))

    # Subgráfico 3D
    ax = fig.add_subplot(gs[0, i], projection="3d")

    # Dibujar la superficie
    surf = ax.plot_surface(
        Y_vals, X_vals, Z_vals,
        rstride=1, cstride=1,
        cmap="viridis",
        edgecolor="none",
        antialiased=True
    )
    surf_list.append(surf)

    ax.set_title(f"Time = {t_val:.2f} s")
    ax.set_xlabel("Y")
    ax.set_ylabel("X")
    ax.set_zlabel("T[K]", labelpad=5, rotation=90)

# Barra de color común
cbar_ax = fig.add_subplot(gs[1, :])
fig.colorbar(surf_list[-1], cax=cbar_ax, orientation="horizontal")

plt.show()
```


![Resultados reportados por @medical_rep en el caso 2D.](images/results_paper.png){#fig-results_fnn fig-align="center" width="550"}
:::

## Comparación con el método de Crank Nickolson
Para tener un valor numérico con el cual conocer al veracidad del modelo se comparó con las predicciones del método de Crank Nickolson

```{python}
import os

# Definir los tiempos a evaluar
times = [0.0, 0.25, 0.5, 0.75, 1.0]

# Rutas de los archivos (modificar según sea necesario)
crank_nick_path = "data/crank_nick.csv"
model_don_path = "data/model_DoN.csv"
output_path = "data/error_comparison.csv"

# Cargar los datos
def load_data(file_path):
    try:
        return pd.read_csv(file_path)
    except FileNotFoundError:
        print(f"Error: No se encontró el archivo {file_path}")
        return None
    except Exception as e:
        print(f"Error al cargar {file_path}: {str(e)}")
        return None

crank_nick_data = load_data(crank_nick_path)
model_don_data = load_data(model_don_path)

if crank_nick_data is None or model_don_data is None:
    exit()

# Función para calcular errores
def calculate_errors(true_data, pred_data, times):
    results = []
    
    for time in times:
        # Filtrar datos por tiempo
        true_subset = true_data[true_data['time'] == time]
        pred_subset = pred_data[pred_data['time'] == time]
        
        if len(true_subset) == 0 or len(pred_subset) == 0:
            print(f"Advertencia: No hay datos para tiempo t={time}")
            continue
        
        # Verificar que las dimensiones coincidan
        if len(true_subset) != len(pred_subset):
            print(f"Advertencia: Número de puntos no coincide para t={time}")
            min_len = min(len(true_subset), len(pred_subset))
            true_subset = true_subset.iloc[:min_len]
            pred_subset = pred_subset.iloc[:min_len]
        
        # Calcular errores para Theta
        theta_true = true_subset['Theta'].values
        theta_pred = pred_subset['Theta'].values
        
        absolute_error = np.abs(theta_true - theta_pred)
        l2_error = np.sqrt(np.sum((theta_true - theta_pred)**2))
        
        results.append({
            'time': time,
            'mean_absolute_error': np.mean(absolute_error),
            'max_absolute_error': np.max(absolute_error),
            'l2_error': l2_error,
            'n_points': len(true_subset)
        })
    
    return pd.DataFrame(results)

# Calcular errores
error_results = calculate_errors(crank_nick_data, model_don_data, times)

# Guardar resultados
error_results.to_csv(output_path, index=False)
```


```{python}
#| label: tbl-errores
#| tbl-cap: "Errores de la red neuronal."
#| echo: false
from tabulate import tabulate

# Convertir a formato markdown para mejor visualización
error_table = tabulate(error_results, headers='keys', tablefmt='pipe', showindex=False)
print(error_table)
```