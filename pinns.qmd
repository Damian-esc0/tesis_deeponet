---
lang: es
execute: 
  freeze: auto
---

# Physics Informed Neural Networks (PINNs) {#sec-pinns}

::: {style="text-align: justify"}
Las Physics-Informed Neural Networks (PINNs) son un enfoque innovador que combina redes neuronales con ecuaciones diferenciales gobernantes para resolver problemas complejos de física [@blechs2021]. A diferencia de métodos tradicionales, las PINNs incorporan directamente las ecuaciones físicas en su función de pérdida mediante diferenciación automática, lo que permite minimizar simultáneamente el error en los datos y el residual de las PDEs [@karniadakis2021]. Esta característica las hace particularmente valiosas en escenarios con datos limitados, donde el conocimiento físico actúa como un regularizador efectivo. La capacidad de aproximación de las PINNs se fundamenta en el teorema de aproximación universal de las redes neuronales, adaptado para incorporar restricciones físicas a través de términos de penalización en la función de optimización [@karniadakis2021].

Como ejemplo, se considera la **ecuación de Burgers para viscocidad**:

$$
\frac{\partial u}{\partial t} + u \frac{\partial u}{\partial x} = \nu \frac{\partial^2 u}{\partial x^2}
$$

Con una condición inicial adecuada y condiciones de contorno de Dirichlet. En la @fig-pinn_graph, la red izquierda (physics-​uninformed) representa el sustituto de la solución de EDP $u(x, t)$, mientras que la red derecha (physics-​informed) describe el residuo de EDP $\frac{\partial u}{\partial t} + u \frac{\partial u}{\partial x} - \nu \frac{\partial^2 u}{\partial x^2} = 0$. La función de pérdida incluye una pérdida supervisada de las mediciones de datos de $u$ de las condiciones iniciales y de contorno, y una pérdida no supervisada de EDP:

$$
\mathcal{L} = w_{\text{data}} \mathcal{L}_{\text{data}} + w_{\text{PDE}} \mathcal{L}_{\text{PDE}}
$$ {#eq-loss-funct}

donde:

\begin{align*}
\mathcal{L}_{\text{data}} =& \frac{1}{N_{\text{data}}} \sum_{i=1}^{N_{\text{data}}} \left( u(x_i, t_i) - u_i \right)^2 \\ \\
\mathcal{L}_{\text{PDE}} =& \frac{1}{N_{\text{PDE}}} \sum_{j=1}^{N_{\text{PDE}}} \left( \frac{\partial u}{\partial t}(x_j, t_j) + u \frac{\partial u}{\partial x}(x_j, t_j) - \nu \frac{\partial^2 u}{\partial x^2}(x_j, t_j) \right)^2
\end{align*}

Aquí, $(x_i, t_i)$ representan puntos donde se conocen valores de la solución y $(x_j, t_j)$ son puntos interiores del dominio. Los pesos $w_{\text{data}}$ y $w_{\text{PDE}}$ equilibran la contribución de cada término. La red se entrena minimizando $\mathcal{L}$ usando optimizadores como Adam o L-BFGS hasta alcanzar un umbral $\varepsilon$ [@karniadakis2021].

Este enfoque permite resolver EDPs (clásicas, fraccionarias o estocásticas) sin mallas, en dominios complejos o con datos escasos y ruidosos, siendo una herramienta flexible y poderosa para la modelación científica.

![**El algoritmo de una PINN.** Construya una red neuronal (NN) $u(x, t; \theta)$ donde $\theta$ representa el conjunto de pesos entrenables $w$ y sesgos $b$, y $\sigma$ representa una función de activación no lineal. Especifique los datos de medición ${x_i, t_i, u_i}$ para $u$ y los puntos residuales ${x_j, t_j}$ para la EDP. Especifique la pérdida $\mathcal{L}$ en la @eq-loss-funct sumando las pérdidas ponderadas de los datos y la EDP. Entrene la NN para encontrar los mejores parámetros $\mathbb{\theta^*}$ minimizando la pérdida $\mathcal{L}$ [@karniadakis2021].](images/PINN_diagram.png){#fig-pinn_graph fig-align="center" width="600" .lightbox}

## Ejemplo de resolución de la ecuación de Burger 1D con deepxde
Dada la ecuación:
$$
\dfrac{\partial u}{\partial t} + u\dfrac{\partial u}{\partial x} = v\dfrac{\partial^2 u}{\partial x^2}, \quad x \in[-1,1], \ t\in[0,1],
$$

con la condición de frontera de Dirichlet y condición inicial:
$$
u(-1,t) = u(1,t) = 0, \quad u(x,0)=-\sin(\pi x).
$$

```{python}
#| output: false
#| warning: false
import deepxde as dde
import numpy as np

# Definir una función para cargar los datos
def gen_testdata():
    data = np.load("data/Burgers.npz")
    t, x, exact = data["t"], data["x"], data["usol"].T
    xx, tt = np.meshgrid(x, t)
    X = np.vstack((np.ravel(xx), np.ravel(tt))).T
    y = exact.flatten()[:, None]
    return X, y

# Definir la PDE
def pde(x, y):
    dy_x = dde.grad.jacobian(y, x, i=0, j=0)
    dy_t = dde.grad.jacobian(y, x, i=0, j=1)
    dy_xx = dde.grad.hessian(y, x, i=0, j=0)
    return dy_t + y * dy_x - 0.01 / np.pi * dy_xx

# Definir los dominios espacial, temporal y juntarlos
geom = dde.geometry.Interval(-1, 1)
timedomain = dde.geometry.TimeDomain(0, 0.99)
geomtime = dde.geometry.GeometryXTime(geom, timedomain)

# Definir la condición de frontera
bc = dde.icbc.DirichletBC(
    geomtime,
    lambda x: 0,
    lambda _, on_boundary: on_boundary)

# Definir la condición inicial
ic = dde.icbc.IC(
    geomtime,
    lambda x: -np.sin(np.pi * x[:, 0:1]),
    lambda _, on_initial: on_initial
)

# Definir la cantidad de puntos en el dominio
data = dde.data.TimePDE(
    geomtime, pde, [bc, ic],
    num_domain=2540,
    num_boundary=80,
    num_initial=160,
    num_test=300
)

# Definir la arquitectura de la red, así como
# su función de activación y el inicializador
net = dde.nn.FNN([2] + [20] * 3 + [1], "tanh", "Glorot normal")
model = dde.Model(data, net)

# Compilar el modelo y entrenarlo
model.compile("adam", lr=1e-3)
losshistory, train_state = model.train(iterations=5000)
model.compile("L-BFGS")
losshistory, train_state = model.train()
#dde.saveplot(losshistory, train_state, issave=False, isplot=True)
```

```{python}
#| fig-cap: "Comparación entre solución real y predicción de la red neuronal para la ecuación de Burger 1D. Debido a su naturaleza unidimensional, es posible plasmar en un eje al tiempo (t) y representar a la función a lo largo de éste como una serie de *fotos* para un instante $t$ dado."
import matplotlib.pyplot as plt 
from mpl_toolkits.mplot3d import Axes3D

X, y_true = gen_testdata()
y_pred = model.predict(X)
f = model.predict(X, operator=pde)

# Extraer componentes de X
x_coords = X[:, 0]  # coordenadas x (espacio)
time = X[:, 1]      # coordenadas t (tiempo)

# Crear figura con dos subgráficos 3D
fig = plt.figure(figsize=(12, 9))
ax1 = fig.add_subplot(121, projection='3d')
ax2 = fig.add_subplot(122, projection='3d')

# Calcular límites comunes para los ejes z
z_min = min(y_true.min(), y_pred.min())
z_max = max(y_true.max(), y_pred.max())

# Gráfico 1: Valores reales
sc1 = ax1.scatter(x_coords, time, y_true, c=y_true,
                cmap='viridis', marker='o', vmin=z_min, vmax=z_max)
ax1.set_xlabel('Posición (x)')
ax1.set_ylabel('Tiempo (t)')
ax1.set_zlabel('u(x,t)')
ax1.set_title('Valores reales de u(x,t)')
ax1.set_zlim([z_min, z_max])

# Gráfico 2: Valores predichos
sc2 = ax2.scatter(x_coords, time, y_pred, c=y_pred,
                cmap='viridis', marker='^', vmin=z_min, vmax=z_max)
ax2.set_xlabel('Posición (x)')
ax2.set_ylabel('Tiempo (t)')
ax2.set_zlabel('u(x,t)')
ax2.set_title('Valores predichos de u(x,t)')
ax2.set_zlim([z_min, z_max])

cbar = fig.colorbar(sc1, ax=(ax1,ax2),
                    shrink=0.9, aspect=90,
                    pad=0.1, orientation='horizontal')
cbar.set_label('Magnitud de u(x,t)')

plt.show()

print("Error relativo L2:", dde.metrics.l2_relative_error(y_true, y_pred))
```


## Comparación con Redes Neuronales Tradicionales

Mientras que las redes neuronales tradicionales dependen exclusivamente de grandes volúmenes de datos etiquetados para su entrenamiento [@karniadakis2021], las PINNs integran el conocimiento físico como parte esencial de su arquitectura [@blechs2021]. Esta diferencia clave permite a las PINNs generar soluciones físicamente consistentes incluso con datos escasos, evitando el sobreajuste común en enfoques puramente basados en datos. Otra ventaja significativa de las PINNs es su naturaleza mesh-free, que contrasta con los métodos numéricos tradicionales como FEM (*Finite Element Method*) o FDM (*Finite Difference Method*) que requieren discretización espacial. Sin embargo, el entrenamiento de PINNs puede ser más desafiante debido a la necesidad de optimizar múltiples objetivos simultáneamente (ajuste a datos y cumplimiento de leyes físicas) [@blechs2021; @karniadakis2021].

:::