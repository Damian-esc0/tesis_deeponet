[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estimación de la temperatura con la ecuacion del Bio-Calor usando DeepONet",
    "section": "",
    "text": "Resumen\nAquí irá el resumen de la tesis.",
    "crumbs": [
      "Resumen"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introducción",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\n\n\nKnuth, Donald E. 1984. «Literate Programming». Comput. J. 27 (2): 97-111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Marco teórcico",
    "section": "",
    "text": "Aqui irá todo lo relacionado a la teoria.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Marco teórcico</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Alessio Borgi, Alessandro De Luca, Eugenio Bugli. 2023. “BioHeat PINNs: Temperature Estimation with Bio-Heat\nEquation using Physics-Informed Neural Networks.” https://github.com/alessioborgi/BioHeat_PINNs/tree/main?tab=readme-ov-file#bioheat-pinns-temperature-estimation-with-bio-heat-equation-using-physics-informed-neural-networks.\n\n\nInstituto Nacional del Cáncer. 2021. “¿Qué es\nel cáncer?” https://www.cancer.gov/espanol/cancer/naturaleza/que-es.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nLu, Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. 2021.\n“DeepXDE: A Deep Learning Library for Solving\nDifferential Equations.” SIAM Review 63 (1): 208–28. https://doi.org/10.1137/19M1274067.\n\n\nNational Cancer Institute. 2021. “Hyperthermia to Treat Cancer.” https://www.cancer.gov/about-cancer/treatment/types/hyperthermia.\n\n\nOrganización Mundial de la Salud. 2022.\n“Cáncer.” https://www.who.int/es/news-room/fact-sheets/detail/cancer.",
    "crumbs": [
      "Referencias"
    ]
  },
  {
    "objectID": "code.html",
    "href": "code.html",
    "title": "3  Código",
    "section": "",
    "text": "El código fuente de la red neuronal es el siguiente\n\n\nCódigo\nimport deepxde as dde\nimport numpy as np\nimport tensorflow as tf\n\n# ------------------------------------------------------------------------------\n# Constants and Parameters\n# ------------------------------------------------------------------------------\n\n# Backend and seed\ndde.backend.set_default_backend(\"tensorflow.compat.v1\")\ndde.config.set_random_seed(123)\n\n# Physical parameters\nheat_coefficient = 1.0\np = 1050\nc = 3639\nkeff = 5\ntf = 1800\nL0 = 0.05\ncb = 3825\nQ = 0\nTM = 45\nTa = 37\nalpha = p * c / keff\n\n# Dimensionless coefficients\na1 = tf / (alpha * L0**2)\na2 = tf * cb / (p * c)\na3 = (tf * Q) / (p * c * (TM - Ta))\n\n# Domain boundaries\nx_initial, x_boundary = 0.0, 1.0\ny_initial, y_boundary = 0.0, 1.0\nt_initial, t_final = 0.0, 1.0\n\n# Dataset configuration\npts_dom = 10\npts_bc = 20\npts_ic = 60\nnum_test = 50\n\n# Sensor grid and function space\nnum_sensors = 4\nsize_cov_matrix = 40\n\n# Network architecture\nwidth_net = 20\nlen_net = 3\nAF = \"elu\"\nk_initializer = \"Glorot normal\"\n\n# Training parameters\nnum_iterations = 3000\nlearning_rate = 9e-4\n\n# ------------------------------------------------------------------------------\n# Geometry and Time Domain\n# ------------------------------------------------------------------------------\n\nspatial_domain = dde.geometry.Rectangle([x_initial, y_initial],\n                                        [x_boundary, y_boundary])\ntime_domain = dde.geometry.TimeDomain(t_initial, t_final)\ngeomtime = dde.geometry.GeometryXTime(spatial_domain, time_domain)\n\n# ------------------------------------------------------------------------------\n# PDE and Conditions\n# ------------------------------------------------------------------------------\n\ndef initial_condition(X):\n    return 0\n\ndef heat_equation(func, u, coords):\n    u_t = dde.grad.jacobian(u, func, i=0, j=2)\n    u_xx = dde.grad.hessian(u, func, i=0, j=0)\n    u_yy = dde.grad.hessian(u, func, i=1, j=1)\n    return a1 * u_t - (u_xx + u_yy) + a2 * u\n\ndef zero_value(X):\n    return 0\n\ndef time_value(X):\n    return X[:, 2]\n\ndef is_on_vertex(x):\n    vertices = np.array([[x_initial, y_initial],\n                         [x_boundary, y_initial],\n                         [x_initial, y_boundary],\n                         [x_boundary, y_boundary]])\n    return any(np.allclose(x, v) for v in vertices)\n\ndef is_initial(X, on_initial):\n    return on_initial and np.isclose(X[2], t_initial)\n\ndef left_boundary(X, on_boundary):\n    spatial = X[0:2]\n    t = X[2]\n    return (\n        on_boundary \n        and np.isclose(spatial[0], x_initial) \n        and not np.isclose(t, t_initial) \n        and not is_on_vertex(spatial)\n    )\n\ndef right_boundary(X, on_boundary):\n    spatial = X[0:2]\n    t = X[2]\n    return (\n        on_boundary \n        and np.isclose(spatial[0], x_boundary) \n        and not np.isclose(t, t_initial) \n        and not is_on_vertex(spatial)\n    )\n\ndef up_low_boundary(X, on_boundary):\n    spatial = X[0:2]\n    t = X[2]\n    return (on_boundary \n    and (np.isclose(spatial[1], y_initial) \n    or np.isclose(spatial[1], y_boundary)) \n    and not np.isclose(t, t_initial) \n    and not is_on_vertex(spatial)\n    )\n\n# Initial and boundary conditions\nic = dde.icbc.IC(geomtime, initial_condition, is_initial)\nleft_bc = dde.icbc.DirichletBC(geomtime, \n                                zero_value, left_boundary)\nright_bc = dde.icbc.NeumannBC(geomtime,\n                                time_value, right_boundary)\nup_low_bc = dde.icbc.NeumannBC(geomtime, \n                                zero_value, up_low_boundary)\n\n# ------------------------------------------------------------------------------\n# Dataset Construction\n# ------------------------------------------------------------------------------\n\npde_data = dde.data.TimePDE(\n    geomtime,\n    heat_equation,\n    [ic, left_bc, right_bc, up_low_bc],\n    num_domain=pts_dom,\n    num_boundary=pts_bc,\n    num_initial=pts_ic,\n)\n\n# ------------------------------------------------------------------------------\n# Sensor Points and Function Space\n# ------------------------------------------------------------------------------\n\nside = np.linspace(x_initial, x_boundary, num_sensors + 1)\nx, y = np.meshgrid(side, side, indexing='xy')\nsensor_pts = np.stack([x.ravel(), y.ravel()], axis=1)\n\nfs = dde.data.function_spaces.GRF2D(N=size_cov_matrix, \n                                    interp=\"linear\")\n\ndata = dde.data.PDEOperatorCartesianProd(\n    pde_data,\n    fs,\n    sensor_pts,\n    num_function=(num_sensors + 1)**2,\n    function_variables=[0, 1],\n    num_test = num_test\n)\n\n# ------------------------------------------------------------------------------\n# Network Definition\n# ------------------------------------------------------------------------------\n\nbranch_layers = [(num_sensors + 1)**2] + len_net * [width_net]\ntrunk_layers = [3] + len_net * [width_net]\n\nnet = dde.nn.DeepONetCartesianProd(\n    branch_layers,\n    trunk_layers,\n    activation=AF,\n    kernel_initializer=k_initializer\n)\n\n# ------------------------------------------------------------------------------\n# Model Compilation and Training\n# ------------------------------------------------------------------------------\n\nmodel = dde.Model(data, net)\nmodel.compile(\"adam\", lr=learning_rate)\nlosshistory, train_state = model.train(iterations=num_iterations)\n\n\nSetting the default backend to \"tensorflow.compat.v1\". You can change it in the ~/.deepxde/config.json file or export the DDE_BACKEND environment variable. Valid options are: tensorflow.compat.v1, tensorflow, pytorch, jax, paddle (all lowercase)\nCompiling model...\nBuilding DeepONetCartesianProd...\n'build' took 0.086493 s\n\n\n\n/home/damian/.local/lib/python3.8/site-packages/deepxde/nn/tensorflow_compat_v1/deeponet.py:549: UserWarning:\n\n`tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n\n/home/damian/.local/lib/python3.8/site-packages/deepxde/nn/tensorflow_compat_v1/deeponet.py:556: UserWarning:\n\n`tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n\n/home/damian/.local/lib/python3.8/site-packages/deepxde/nn/tensorflow_compat_v1/deeponet.py:570: UserWarning:\n\n`tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n\n\n\n'compile' took 26.017443 s\n\nTraining model...\n\nStep      Train loss                                            Test loss                                             Test metric\n0         [2.57e+00, 4.03e-01, 4.13e-01, 6.71e-01, 1.05e+00]    [2.19e+00, 2.88e-01, 4.46e-01, 6.22e-01, 4.51e-01]    []  \n1000      [9.69e-03, 4.88e-03, 7.58e-04, 2.86e-02, 3.63e-04]    [1.25e-02, 5.13e-03, 1.31e-03, 2.96e-02, 9.29e-04]    []  \n2000      [2.79e-03, 2.39e-03, 2.50e-04, 2.61e-02, 1.67e-04]    [4.95e-03, 2.43e-03, 6.75e-04, 2.66e-02, 2.56e-04]    []  \n3000      [1.81e-03, 9.38e-04, 8.22e-05, 2.58e-02, 6.91e-05]    [3.42e-03, 1.02e-03, 4.12e-04, 2.62e-02, 7.67e-05]    []  \n\nBest model at step 3000:\n  train loss: 2.87e-02\n  test loss: 3.11e-02\n  test metric: []\n\n'train' took 36.754497 s\n\n\n\nEl historial de perdida para el entrenamiento obtenido es el siguiente:\n\n\nCódigo\nimport plotly.graph_objects as go\n# Nombres de las componentes del loss\nloss_labels = [\n    \"PDE residual loss\",\n    \"Initial‐condition loss\",\n    \"Left‐boundary (Dirichlet) loss\",\n    \"Right‐boundary (Neumann) loss\",\n    \"Top/Bottom‐boundary (Neumann) loss\"\n]\n\n# Extraer pasos y pérdida de entrenamiento\nsteps = losshistory.steps\ntrain_loss = np.array(losshistory.loss_train)  # Asegurarse de que sea array NumPy\n\n# Crear figura\nfig = go.Figure()\n\n# Agregar cada componente del loss\nfor i in range(train_loss.shape[1]):\n    fig.add_trace(go.Scatter(\n        x=steps,\n        y=train_loss[:, i],\n        mode='lines',\n        name=loss_labels[i]\n    ))\n\n# Layout con escala logarítmica y notación científica\nfig.update_layout(\n    title=\"Training Loss history\",\n    xaxis=dict(\n        title=\"Iteration\",\n        tickformat=\".1e\"  # Notación científica\n    ),\n    yaxis=dict(\n        title=\"Loss\",\n        type=\"log\",\n        tickformat=\".1e\"  # Notación científica\n    ),\n    template=\"plotly_white\",\n    legend=dict(x=0.99, y=0.99),\n    font=dict(size=14)\n)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigura 3.1: Historial de perdida en el entrenamiento de la red neuronal.\n\n\n\n\nEl historial de perdida para el conjunto de prueba es el siguiente:\n\n\nCódigo\nimport plotly.graph_objects as go\n# Nombres de las componentes del loss\nloss_labels = [\n    \"PDE residual loss\",\n    \"Initial‐condition loss\",\n    \"Left‐boundary (Dirichlet) loss\",\n    \"Right‐boundary (Neumann) loss\",\n    \"Top/Bottom‐boundary (Neumann) loss\"\n]\n\n# Extraer pasos y pérdida de entrenamiento\nsteps = losshistory.steps\ntest_loss = np.array(losshistory.loss_test)  \n\n# Crear figura\nfig = go.Figure()\n\n# Agregar cada componente del loss\nfor i in range(test_loss.shape[1]):\n    fig.add_trace(go.Scatter(\n        x=steps,\n        y=test_loss[:, i],\n        mode='lines',\n        name=loss_labels[i]\n    ))\n\n# Layout con escala logarítmica y notación científica\nfig.update_layout(\n    title=\"Test Loss history\",\n    xaxis=dict(\n        title=\"Iteration\",\n        tickformat=\".1e\"  # Notación científica\n    ),\n    yaxis=dict(\n        title=\"Loss\",\n        type=\"log\",\n        tickformat=\".1e\"  # Notación científica\n    ),\n    template=\"plotly_white\",\n    legend=dict(x=0.99, y=0.99),\n    font=dict(size=14)\n)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigura 3.2: Historial de perdida en el conjunto de prueba de la red neuronal.\n\n\n\n\nA continuación, los valores predichos por la red neuronal a tiempos t(s) de 0, 0.25, 0.5, 0.75 y 1.0.\n\n\nCódigo\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.gridspec as gridspec\n\n# Times at which to evaluate the model\ntimes = [0.0, 0.25, 0.5, 0.75, 1.0]\n\n# Generate a grid of (x, y) points\nnum_points = 25\nx = np.linspace(0, 1, num_points)\ny = np.linspace(0, 1, num_points)\nX, Y = np.meshgrid(x, y)\n\n# Create a figure and a GridSpec layout.\n# We reserve one row at the bottom for the colorbar.\nncols = len(times)\nfig = plt.figure(figsize=((5 * ncols) +1, 6))\ngs = gridspec.GridSpec(2, ncols, height_ratios=[10, 1], hspace=0.3)\n\n# Create a list to store the surface plots for the color bar.\nsurf_list = []\n\nfor i, t_val in enumerate(times):\n    # Create trunk input for the model: shape (num_points^2, 3)\n    points = np.vstack((X.flatten(), Y.flatten(), t_val * np.ones_like(X.flatten()))).T\n\n    # Create branch input: for your constant zero initial condition,\n    # just use an array of zeros with shape (1, num_sensors)\n    branch_input = np.zeros((1, sensor_pts.shape[0]))\n\n    # Predict\n    predicted = model.predict((branch_input, points))\n    predicted = predicted.flatten()\n    # Reshape to 2D\n    Z = predicted.reshape(X.shape)\n\n    # 3D subplot\n    ax = fig.add_subplot(gs[0, i], projection=\"3d\")\n\n    # Plot surface\n    surf = ax.plot_surface(\n        Y, X, Z,\n        rstride=1, cstride=1,\n        cmap=\"viridis\",\n        edgecolor=\"none\",\n        antialiased=True\n    )\n    surf_list.append(surf)\n\n    ax.set_title(f\"Time = {t_val:.2f} s\")\n    ax.set_xlabel(\"Y\")\n    ax.set_ylabel(\"X\")\n    ax.set_zlabel(\"T[K]\")\n\n# Create a single color bar below all subplots\n# We take the mappable from the last subplot (or average from one)\ncbar_ax = fig.add_subplot(gs[1, :])\n# Use the mappable from the last subplot; orientation horizontal.\nfig.colorbar(surf_list[-1], cax=cbar_ax, orientation=\"horizontal\")\n\n#plt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigura 3.3: Predicciones de la red neuronal a distintos tiempos.\n\n\n\n\n\n\n\n\n\n\n\nFigura 3.4: Resultados reportados por Alessio Borgi (2023) en el caso 2D.\n\n\n\n\n\n\n\nAlessio Borgi, Alessandro De Luca, Eugenio Bugli. 2023. «BioHeat PINNs: Temperature Estimation with Bio-Heat Equation using Physics-Informed Neural Networks». https://github.com/alessioborgi/BioHeat_PINNs/tree/main?tab=readme-ov-file#bioheat-pinns-temperature-estimation-with-bio-heat-equation-using-physics-informed-neural-networks.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Código</span>"
    ]
  },
  {
    "objectID": "marco_teorico.html",
    "href": "marco_teorico.html",
    "title": "2  Marco teórcico",
    "section": "",
    "text": "Aqui irá todo lo relacionado a la teoria.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Marco teórcico</span>"
    ]
  },
  {
    "objectID": "estudio_caso.html",
    "href": "estudio_caso.html",
    "title": "Estudio de caso",
    "section": "",
    "text": "La Organización Mundial de la Salud (2022) en su página web define Cáncer como:\n\n«Cáncer» es un término genérico utilizado para designar un amplio grupo de enfermedades que pueden afectar a cualquier parte del organismo; también se habla de «tumores malignos» o «neoplasias malignas». Una característica definitoria del cáncer es la multiplicación rápida de células anormales que se extienden más allá de sus límites habituales y pueden invadir partes adyacentes del cuerpo o propagarse a otros órganos, en un proceso que se denomina «metástasis». La extensión de las metástasis es la principal causa de muerte por la enfermedad.\n\nPor su parte Instituto Nacional del Cáncer (2021) aporta lo siguiente:\n\nEs posible que el cáncer comience en cualquier parte del cuerpo humano, formado por billones de células. En condiciones normales, las células humanas se forman y se multiplican (mediante un proceso que se llama división celular) para formar células nuevas a medida que el cuerpo las necesita. Cuando las células envejecen o se dañan, mueren y las células nuevas las reemplazan. A veces el proceso no sigue este orden y las células anormales o células dañadas se forman y se multiplican cuando no deberían. Estas células tal vez formen tumores, que son bultos de tejido. Los tumores son cancerosos (malignos) o no cancerosos (benignos).\n\n\n\n\n\n\n\nFigura 1: Una célula de cáncer de seno que se multiplica, Instituto Nacional del Cáncer (2021).\n\n\n\nÉsta enfermedad es la principal causa de muerte a nivel mundial, solo en 2020 arrebató casi 10 millones de vidas y según datos de Organización Mundial de la Salud (2022) los cánceres más comunes en 2020 fueron:\n\nDe mama (2.26 millones de casos)\nDe pulmón (2.21 millones de casos)\nDe colon (1.93 millones de casos)\nDe próstata (1.41 millones de casos)\nDe piel (distinto del melanoma) (1.20 millones de casos)\nGástrico (1.09 millones de casos)\n\nEs ante este panorama que distintos tratamientos surgen con el objetivo de erradicar la enfermedad siempre que se tenga una detección oportuna. Uno de dichos tratamientos es la hipertermia, según en el National Cancer Institute (2021) es un método que consiste en calentar el tejido corporal hasta los 39-45 °C para ayudar a erradicar células cancerígenas con pequeñas o nulas lesiones en el tejido sano. La hipertermia también es llamada terapia térmica o termoterapia.\nUno de los principales retos de este tratamiento es la creación de un modelo óptimo que se adecue al comportamiento de la transferencia de calor que se hace a los tejidos con el fin de dañar únicamente el área en el que se encuentran las célular cancerígenas, es por ello que los modelos de integencia artificial y más precisamente las PINN’s (aqui irá una cita) surgen como posible solución a este reto.\nEl presente estudio utilizó como punto de partida el trabajo realizado por Alessio Borgi (2023) para modelar el calentamiento del tejido corporal usando la ecuación del Bio-Calor en dos dimensiones.\n\n\n\n\n\nAlessio Borgi, Alessandro De Luca, Eugenio Bugli. 2023. «BioHeat PINNs: Temperature Estimation with Bio-Heat Equation using Physics-Informed Neural Networks». https://github.com/alessioborgi/BioHeat_PINNs/tree/main?tab=readme-ov-file#bioheat-pinns-temperature-estimation-with-bio-heat-equation-using-physics-informed-neural-networks.\n\n\nInstituto Nacional del Cáncer. 2021. «¿Qué es el cáncer?» https://www.cancer.gov/espanol/cancer/naturaleza/que-es.\n\n\nNational Cancer Institute. 2021. «Hyperthermia to Treat Cancer». https://www.cancer.gov/about-cancer/treatment/types/hyperthermia.\n\n\nOrganización Mundial de la Salud. 2022. «Cáncer». https://www.who.int/es/news-room/fact-sheets/detail/cancer.",
    "crumbs": [
      "Estudio de caso"
    ]
  },
  {
    "objectID": "metodologia.html#diseño-del-modelo",
    "href": "metodologia.html#diseño-del-modelo",
    "title": "2  Metodología",
    "section": "2.2 Diseño del modelo",
    "text": "2.2 Diseño del modelo\nEl lenguaje seleccionado fué Python, a su vez el código se basa enteramente en la librería Deepxde creada por Lu et al. (2021) la cual está directamente enfocada a resolver ecuaciones diferenciales, se usó además como backend tensorflow_compat_v1 siendo su elección debida únicamente a la familiarización previa que se tenía con ella. Finalmente el entorno donde se programó y optimizó el código fué en Google Colab ya que la potencia de cómputo ofrecida por la plataforma era necesaria para ejecutar el modelo.",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metodología</span>"
    ]
  },
  {
    "objectID": "metodologia.html#implementación-del-modelo",
    "href": "metodologia.html#implementación-del-modelo",
    "title": "2  Metodología",
    "section": "2.3 Implementación del modelo",
    "text": "2.3 Implementación del modelo\nUna vez creado el código que resuelve la ecuación del Bio-Calor, se ajustaron los hiperparámetros tales como cantidad de épocas de entrenamiento, el ratio de aprendizaje, la función de activación y el inicializador en base al trabajo de Alessio Borgi (2023).",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metodología</span>"
    ]
  },
  {
    "objectID": "metodologia.html#evaluación-del-modelo",
    "href": "metodologia.html#evaluación-del-modelo",
    "title": "2  Metodología",
    "section": "2.4 Evaluación del modelo",
    "text": "2.4 Evaluación del modelo\nSe llevó a cabo una evaluación del modelo al darle como entrada un conjunto de datos que no había visto y posteriormente obtener como salida sus predicciones, con ellas se elaboraron gráficas claras y detalladas de su pronóstico en el intervalo de tiempo y espacio especificados.",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metodología</span>"
    ]
  },
  {
    "objectID": "metodologia.html#comparación-de-resultados",
    "href": "metodologia.html#comparación-de-resultados",
    "title": "2  Metodología",
    "section": "2.5 Comparación de resultados",
    "text": "2.5 Comparación de resultados\nLos resultados obtenidos de la evaluación del modelo fueron comparados con los del trabajo de Alessio Borgi (2023) para determinar su eficacia predictiva relativa. Se analizaron las fortalezas y debilidades del modelo en función de su desempeño en la predicción de las variables de interés.",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metodología</span>"
    ]
  },
  {
    "objectID": "metodologia.html#análisis-y-conclusión",
    "href": "metodologia.html#análisis-y-conclusión",
    "title": "2  Metodología",
    "section": "2.6 Análisis y conclusión",
    "text": "2.6 Análisis y conclusión\nFinalmente, se realizó un análisis detallado de los resultados obtenidos para extraer conclusiones significativas. Se proporcionaron recomendaciones basadas en los hallazgos del estudio, lo que permitió establecer un marco para interpretaciones analíticas profundas y recomendaciones bien fundamentadas en la sección de conclusiones del estudio.\nEste enfoque metodológico proporcionó una base sólida para los resultados obtenidos, asegurando la integridad y la calidad del análisis realizado en el estudio.",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metodología</span>"
    ]
  },
  {
    "objectID": "predicciones.html",
    "href": "predicciones.html",
    "title": "3  Predicciones del modelo",
    "section": "",
    "text": "Conforme se ha referido previamente, se creó el modelo utilizando Deepxde como base. Resulta relevante descatacar que se empleó la versión 1.10.1 de dicha librería. A continuación se presenta el código fuente de la red neuronal\n\n\nCódigo\nimport deepxde as dde\nimport numpy as np\nimport tensorflow as tf\n\n# ------------------------------------------------------------------------------\n# Constants and Parameters\n# ------------------------------------------------------------------------------\n\n# Backend and seed\ndde.backend.set_default_backend(\"tensorflow.compat.v1\")\ndde.config.set_random_seed(123)\n\n# Physical parameters\nheat_coefficient = 1.0\np = 1050\nc = 3639\nkeff = 5\ntf = 1800\nL0 = 0.05\ncb = 3825\nQ = 0\nTM = 45\nTa = 37\nalpha = p * c / keff\n\n# Dimensionless coefficients\na1 = tf / (alpha * L0**2)\na2 = tf * cb / (p * c)\na3 = (tf * Q) / (p * c * (TM - Ta))\n\n# Domain boundaries\nx_initial, x_boundary = 0.0, 1.0\ny_initial, y_boundary = 0.0, 1.0\nt_initial, t_final = 0.0, 1.0\n\n# Dataset configuration\npts_dom = 10\npts_bc = 20\npts_ic = 60\nnum_test = 20\n\n# Sensor grid and function space\nnum_sensors = 4\nsize_cov_matrix = 40\n\n# Network architecture\nwidth_net = 20\nlen_net = 3\nAF = \"elu\"\nk_initializer = \"Glorot normal\"\n\n# Training parameters\nnum_iterations = 3000\nlearning_rate = 2e-3\ndecay_rate = 0.05\ndecay_steps = 1000\n\n# ------------------------------------------------------------------------------\n# Geometry and Time Domain\n# ------------------------------------------------------------------------------\n\nspatial_domain = dde.geometry.Rectangle([x_initial, y_initial],\n                                        [x_boundary, y_boundary])\ntime_domain = dde.geometry.TimeDomain(t_initial, t_final)\ngeomtime = dde.geometry.GeometryXTime(spatial_domain, time_domain)\n\n# ------------------------------------------------------------------------------\n# PDE and Conditions\n# ------------------------------------------------------------------------------\n\ndef initial_condition(X):\n    return 0\n\ndef heat_equation(func, u, coords):\n    u_t = dde.grad.jacobian(u, func, i=0, j=2)\n    u_xx = dde.grad.hessian(u, func, i=0, j=0)\n    u_yy = dde.grad.hessian(u, func, i=1, j=1)\n    return a1 * u_t - (u_xx + u_yy) + a2 * u\n\ndef zero_value(X):\n    return 0\n\ndef time_value(X):\n    return X[:, 2]\n\ndef is_on_vertex(x):\n    vertices = np.array([[x_initial, y_initial],\n                         [x_boundary, y_initial],\n                         [x_initial, y_boundary],\n                         [x_boundary, y_boundary]])\n    return any(np.allclose(x, v) for v in vertices)\n\ndef is_initial(X, on_initial):\n    return on_initial and np.isclose(X[2], t_initial)\n\ndef left_boundary(X, on_boundary):\n    spatial = X[0:2]\n    t = X[2]\n    return (\n        on_boundary \n        and np.isclose(spatial[0], x_initial) \n        and not np.isclose(t, t_initial) \n        and not is_on_vertex(spatial)\n    )\n\ndef right_boundary(X, on_boundary):\n    spatial = X[0:2]\n    t = X[2]\n    return (\n        on_boundary \n        and np.isclose(spatial[0], x_boundary) \n        and not np.isclose(t, t_initial) \n        and not is_on_vertex(spatial)\n    )\n\ndef up_low_boundary(X, on_boundary):\n    spatial = X[0:2]\n    t = X[2]\n    return (on_boundary \n    and (np.isclose(spatial[1], y_initial) \n    or np.isclose(spatial[1], y_boundary)) \n    and not np.isclose(t, t_initial) \n    and not is_on_vertex(spatial)\n    )\n\n# Initial and boundary conditions\nic = dde.icbc.IC(geomtime, initial_condition, is_initial)\nleft_bc = dde.icbc.DirichletBC(geomtime, \n                                zero_value, left_boundary)\nright_bc = dde.icbc.NeumannBC(geomtime,\n                                time_value, right_boundary)\nup_low_bc = dde.icbc.NeumannBC(geomtime, \n                                zero_value, up_low_boundary)\n\n# ------------------------------------------------------------------------------\n# Dataset Construction\n# ------------------------------------------------------------------------------\n\npde_data = dde.data.TimePDE(\n    geomtime,\n    heat_equation,\n    [ic, left_bc, right_bc, up_low_bc],\n    num_domain=pts_dom,\n    num_boundary=pts_bc,\n    num_initial=pts_ic,\n    num_test = num_test \n)\n\n# ------------------------------------------------------------------------------\n# Sensor Points and Function Space\n# ------------------------------------------------------------------------------\n\nside = np.linspace(x_initial, x_boundary, num_sensors + 1)\nx, y = np.meshgrid(side, side, indexing='xy')\nsensor_pts = np.stack([x.ravel(), y.ravel()], axis=1)\n\nfs = dde.data.function_spaces.GRF2D(N=size_cov_matrix, \n                                    interp=\"linear\")\n\ndata = dde.data.PDEOperatorCartesianProd(\n    pde_data,\n    fs,\n    sensor_pts,\n    num_function=(num_sensors + 1)**2,\n    function_variables=[0, 1],\n    num_test=num_test\n)\n\n# ------------------------------------------------------------------------------\n# Network Definition\n# ------------------------------------------------------------------------------\n\nbranch_layers = [(num_sensors + 1)**2] + len_net * [width_net]\ntrunk_layers = [3] + len_net * [width_net]\n\nnet = dde.nn.DeepONetCartesianProd(\n    branch_layers,\n    trunk_layers,\n    activation=AF,\n    kernel_initializer=k_initializer\n)\n\n# ------------------------------------------------------------------------------\n# Model Compilation and Training\n# ------------------------------------------------------------------------------\n\nmodel = dde.Model(data, net)\nmodel.compile(\"adam\", lr=learning_rate, decay=(\"inverse time\", decay_steps, decay_rate))\nlosshistory, train_state = model.train(iterations=num_iterations)\n\n# Fine-tuning with LBFGS optimizer\nmodel.compile(\"L-BFGS\")\nlosshistory, train_state = model.train()\n\n\n2025-05-13 20:43:08.981128: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2025-05-13 20:43:09.045810: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2025-05-13 20:43:09.046752: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-05-13 20:43:09.841416: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nUsing backend: tensorflow.compat.v1\nOther supported backends: tensorflow, pytorch, jax, paddle.\npaddle supports more examples now and is recommended.\n\n\nWARNING:tensorflow:From /home/damian/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\nInstructions for updating:\nnon-resource variables are not supported in the long term\nSetting the default backend to \"tensorflow.compat.v1\". You can change it in the ~/.deepxde/config.json file or export the DDE_BACKEND environment variable. Valid options are: tensorflow.compat.v1, tensorflow, pytorch, jax, paddle (all lowercase)\nWarning: 5 points required, but 9 points sampled.\nWarning: 20 points required, but 36 points sampled.\nCompiling model...\nBuilding DeepONetCartesianProd...\n'build' took 0.100526 s\n\n\n\n/home/damian/.local/lib/python3.8/site-packages/deepxde/nn/tensorflow_compat_v1/deeponet.py:549: UserWarning:\n\n`tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n\n/home/damian/.local/lib/python3.8/site-packages/deepxde/nn/tensorflow_compat_v1/deeponet.py:556: UserWarning:\n\n`tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n\n/home/damian/.local/lib/python3.8/site-packages/deepxde/nn/tensorflow_compat_v1/deeponet.py:570: UserWarning:\n\n`tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n\n\n\n'compile' took 13.902830 s\n\n\n\n2025-05-13 20:43:25.253266: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n\n\nTraining model...\n\nStep      Train loss                                            Test loss                                             Test metric\n0         [2.57e+00, 4.03e-01, 4.13e-01, 6.71e-01, 1.05e+00]    [2.46e+00, 1.26e-01, 3.07e-01, 4.89e-01, 3.14e-01]    []  \n1000      [3.97e-03, 3.15e-03, 3.81e-04, 2.63e-02, 1.86e-04]    [4.48e-03, 3.56e-03, 7.66e-04, 2.76e-02, 4.51e-04]    []  \n2000      [1.23e-03, 1.01e-03, 1.47e-04, 2.58e-02, 7.37e-05]    [3.65e-03, 1.18e-03, 4.23e-04, 2.68e-02, 1.21e-04]    []  \n3000      [8.29e-04, 3.99e-04, 7.91e-05, 2.57e-02, 2.38e-05]    [2.69e-03, 5.61e-04, 1.16e-04, 2.63e-02, 3.19e-05]    []  \n\nBest model at step 3000:\n  train loss: 2.71e-02\n  test loss: 2.97e-02\n  test metric: []\n\n'train' took 32.578577 s\n\nCompiling model...\n'compile' took 29.609841 s\n\nTraining model...\n\nStep      Train loss                                            Test loss                                             Test metric\n3000      [8.29e-04, 3.99e-04, 7.91e-05, 2.57e-02, 2.38e-05]    [2.69e-03, 5.61e-04, 1.16e-04, 2.63e-02, 3.19e-05]    []  \nINFO:tensorflow:Optimization terminated with:\n  Message: CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH\n  Objective function value: 0.026917\n  Number of iterations: 9\n  Number of functions evaluations: 48\n3048      [7.24e-04, 4.15e-04, 3.24e-05, 2.57e-02, 2.32e-05]    [2.99e-03, 5.67e-04, 9.16e-05, 2.62e-02, 3.12e-05]    []  \n\nBest model at step 3048:\n  train loss: 2.69e-02\n  test loss: 2.99e-02\n  test metric: []\n\n'train' took 10.521231 s\n\n\n\nEl historial de perdida para el entrenamiento obtenido es el siguiente:\n\n\nCódigo\nimport plotly.graph_objects as go\n# Nombres de las componentes del loss\nloss_labels = [\n    \"PDE residual loss\",\n    \"Initial‐condition loss\",\n    \"Left‐boundary (Dirichlet) loss\",\n    \"Right‐boundary (Neumann) loss\",\n    \"Top/Bottom‐boundary (Neumann) loss\"\n]\n\n# Extraer pasos y pérdida de entrenamiento\nsteps = losshistory.steps\ntrain_loss = np.array(losshistory.loss_train)  # Asegurarse de que sea array NumPy\n\n# Crear figura\nfig = go.Figure()\n\n# Agregar cada componente del loss\nfor i in range(train_loss.shape[1]):\n    fig.add_trace(go.Scatter(\n        x=steps,\n        y=train_loss[:, i],\n        mode='lines',\n        name=loss_labels[i]\n    ))\n\n# Layout con escala logarítmica y notación científica\nfig.update_layout(\n    title=\"Training Loss history\",\n    xaxis=dict(\n        title=\"Iteration\",\n        tickformat=\".1e\"  # Notación científica\n    ),\n    yaxis=dict(\n        title=\"Loss\",\n        type=\"log\",\n        tickformat=\".1e\"  # Notación científica\n    ),\n    template=\"plotly_white\",\n    legend=dict(x=0.99, y=0.99),\n    font=dict(size=14)\n)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigura 3.1: Historial de perdida en el entrenamiento de la red neuronal.\n\n\n\n\nEl historial de perdida para el conjunto de prueba es el siguiente:\n\n\nCódigo\nimport plotly.graph_objects as go\n# Nombres de las componentes del loss\nloss_labels = [\n    \"PDE residual loss\",\n    \"Initial‐condition loss\",\n    \"Left‐boundary (Dirichlet) loss\",\n    \"Right‐boundary (Neumann) loss\",\n    \"Top/Bottom‐boundary (Neumann) loss\"\n]\n\n# Extraer pasos y pérdida del conjunto de prueba\nsteps = losshistory.steps\ntest_loss = np.array(losshistory.loss_test)  \n\n# Crear figura\nfig = go.Figure()\n\n# Agregar cada componente del loss\nfor i in range(test_loss.shape[1]):\n    fig.add_trace(go.Scatter(\n        x=steps,\n        y=test_loss[:, i],\n        mode='lines',\n        name=loss_labels[i]\n    ))\n\n# Layout con escala logarítmica y notación científica\nfig.update_layout(\n    title=\"Test Loss history\",\n    xaxis=dict(\n        title=\"Iteration\",\n        tickformat=\".1e\"  # Notación científica\n    ),\n    yaxis=dict(\n        title=\"Loss\",\n        type=\"log\",\n        tickformat=\".1e\"  # Notación científica\n    ),\n    template=\"plotly_white\",\n    legend=dict(x=0.99, y=0.99),\n    font=dict(size=14)\n)\n\nfig.show()\n\n\n\n\n                                                \n\n\nFigura 3.2: Historial de perdida en el conjunto de prueba de la red neuronal.\n\n\n\n\nA continuación, los valores predichos por la red neuronal a tiempos t(s) de 0.0, 0.25, 0.50, 0.75 y 1.0.\n\n\nCódigo\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.gridspec as gridspec\n\n# Times at which to evaluate the model\ntimes = [0.0, 0.25, 0.5, 0.75, 1.0]\n\n# Generate a grid of (x, y) points\nnum_points = 25\nx = np.linspace(0, 1, num_points)\ny = np.linspace(0, 1, num_points)\nX, Y = np.meshgrid(x, y)\n\n# Create a figure and a GridSpec layout.\n# We reserve one row at the bottom for the colorbar.\nncols = len(times)\nfig = plt.figure(figsize=((5 * ncols) +1, 6))\ngs = gridspec.GridSpec(2, ncols, height_ratios=[10, 1], hspace=0.3)\n\n# Create a list to store the surface plots for the color bar.\nsurf_list = []\n\nfor i, t_val in enumerate(times):\n    # Create trunk input for the model: shape (num_points^2, 3)\n    points = np.vstack((X.flatten(), Y.flatten(), t_val * np.ones_like(X.flatten()))).T\n\n    # Create branch input: for your constant zero initial condition,\n    # just use an array of zeros with shape (1, num_sensors)\n    branch_input = np.zeros((1, sensor_pts.shape[0]))\n\n    # Predict\n    predicted = model.predict((branch_input, points))\n    predicted = predicted.flatten()\n    # Reshape to 2D\n    Z = predicted.reshape(X.shape)\n\n    # 3D subplot\n    ax = fig.add_subplot(gs[0, i], projection=\"3d\")\n\n    # Plot surface\n    surf = ax.plot_surface(\n        Y, X, Z,\n        rstride=1, cstride=1,\n        cmap=\"viridis\",\n        edgecolor=\"none\",\n        antialiased=True\n    )\n    surf_list.append(surf)\n\n    ax.set_title(f\"Time = {t_val:.2f} s\")\n    ax.set_xlabel(\"Y\")\n    ax.set_ylabel(\"X\")\n    ax.set_zlabel(\"T[K]\")\n\n# Create a single color bar below all subplots\n# We take the mappable from the last subplot (or average from one)\ncbar_ax = fig.add_subplot(gs[1, :])\n# Use the mappable from the last subplot; orientation horizontal.\nfig.colorbar(surf_list[-1], cax=cbar_ax, orientation=\"horizontal\")\n\n#plt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigura 3.3: Predicciones de la red neuronal a distintos tiempos.\n\n\n\n\n\n\n\n\n\n\n\nFigura 3.4: Resultados reportados por Alessio Borgi (2023) en el caso 2D.\n\n\n\n\n\n\n\nAlessio Borgi, Alessandro De Luca, Eugenio Bugli. 2023. «BioHeat PINNs: Temperature Estimation with Bio-Heat Equation using Physics-Informed Neural Networks». https://github.com/alessioborgi/BioHeat_PINNs/tree/main?tab=readme-ov-file#bioheat-pinns-temperature-estimation-with-bio-heat-equation-using-physics-informed-neural-networks.",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Predicciones del modelo</span>"
    ]
  },
  {
    "objectID": "metodologia.html#margen-de-mejora",
    "href": "metodologia.html#margen-de-mejora",
    "title": "2  Metodología",
    "section": "2.1 Margen de mejora",
    "text": "2.1 Margen de mejora\nYa que se parte del trabajo de Alessio Borgi (2023), se examinó que dos de los puntos a mejorar de la red neuronal que plantearon son:\n\nDesarrollar nuevas arquitecturas para la red neuronal y explorar nuevas configuraciones\nCombinar las fortalezas de los algoritmos de optimización Adam y L-BFGS para mejorar la velocidad de convergencia y la precisión\n\nTenindo los anteriores puntos en cuenta, se procedió a abordarlos e implementarlos dentro del diseño del modelo.",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metodología</span>"
    ]
  },
  {
    "objectID": "metodologia.html#aportaciones-del-modelo",
    "href": "metodologia.html#aportaciones-del-modelo",
    "title": "2  Metodología",
    "section": "2.1 Aportaciones del modelo",
    "text": "2.1 Aportaciones del modelo\nYa que se parte del trabajo de Alessio Borgi (2023), se examinó que dos de los puntos a mejorar de la red neuronal que plantearon son:\n\nDesarrollar nuevas arquitecturas para la red neuronal y explorar nuevas configuraciones\nCombinar las fortalezas de los algoritmos de optimización Adam y L-BFGS para mejorar la velocidad de convergencia y la precisión\n\nTenindo los anteriores puntos en cuenta, se procedió a abordarlos e implementarlos dentro del diseño del modelo.",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Metodología</span>"
    ]
  }
]