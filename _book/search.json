[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Estimación de la temperatura con la ecuación del Bio-Calor usando DeepONet",
    "section": "",
    "text": "Resumen\nAquí irá el resumen de la tesis.",
    "crumbs": [
      "Resumen"
    ]
  },
  {
    "objectID": "partial_diff_eq.html#ecuación-diferencial-parcial-lineal",
    "href": "partial_diff_eq.html#ecuación-diferencial-parcial-lineal",
    "title": "2  Ecuaciones diferenciales parciales",
    "section": "2.1 Ecuación diferencial parcial lineal",
    "text": "2.1 Ecuación diferencial parcial lineal\nSi dejamos que \\(u\\) denote la variable dependiente y que \\(x\\) e \\(y\\) representen las variables independientes, entonces la forma general de una ecuación diferencial parcial lineal de segundo orden está dada por:\n\\[\nA \\dfrac{\\partial^2 u}{\\partial x^2} + B \\dfrac{\\partial^2 u}{\\partial x \\, \\partial y} + C \\dfrac{\\partial^2 u}{\\partial y^2} + D \\dfrac{\\partial u}{\\partial x} + E \\dfrac{\\partial u}{\\partial y} + F u = G,\n\\tag{2.1}\\]\ndonde los coeficientes \\(A, B, C, \\dots, G\\) son funciones de \\(x\\) e \\(y\\). Cuando \\(G(x, y) = 0\\), la ecuación 2.1 se denomina homogénea; de lo contrario, es no homogénea. Por ejemplo, las ecuaciones lineales:\n\\[\n\\dfrac{\\partial^2 u}{\\partial x^2} + \\dfrac{\\partial^2 u}{\\partial y^2} = 0 \\quad\\text{y} \\quad \\dfrac{\\partial^2 u}{\\partial x^2} - \\dfrac{\\partial u}{\\partial y} = x y\n\\] son homogénea y no homogénea, respectivamente.",
    "crumbs": [
      "Preliminares",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Ecuaciones diferenciales parciales</span>"
    ]
  },
  {
    "objectID": "partial_diff_eq.html#solucion-de-una-pde",
    "href": "partial_diff_eq.html#solucion-de-una-pde",
    "title": "2  Ecuaciones diferenciales parciales",
    "section": "2.2 Solucion de una PDE",
    "text": "2.2 Solucion de una PDE\nUna solución de una ecuación diferencial parcial es una función \\(u(x, y)\\) de dos variables independientes que posee todas las derivadas parciales que aparecen en la ecuación y que satisface dicha ecuación en alguna región del plano \\(xy\\).\nNo es lo habitual examinar los procedimientos para encontrar soluciones generales de ecuaciones diferenciales parciales lineales. No solo porque suele ser difícil obtener una solución general de una EDP lineal de segundo orden, sino que una solución general generalmente no es tan útil en aplicaciones prácticas. Por lo tanto, el enfoque común es el de encontrar soluciones particulares de algunas de las EDPs lineales más importantes, es decir, ecuaciones que aparecen en muchas aplicaciones.",
    "crumbs": [
      "Preliminares",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Ecuaciones diferenciales parciales</span>"
    ]
  },
  {
    "objectID": "partial_diff_eq.html#separación-de-variables",
    "href": "partial_diff_eq.html#separación-de-variables",
    "title": "2  Ecuaciones diferenciales parciales",
    "section": "2.3 Separación de variables",
    "text": "2.3 Separación de variables\nAunque existen varios métodos que pueden intentarse para encontrar soluciones particulares de una EDP lineal, uno de los métodos más comunes se llama método de separación de variables. En este método buscamos una solución particular de la forma de un producto de una función de \\(x\\) y una función de \\(y\\):\n\\[\nu(x, y) = X(x)Y(y).\n\\]\nCon esta suposición, a veces es posible reducir una EDP lineal en dos variables a dos ecuaciones diferenciales ordinarias (ODEs). Para este fin, observamos que:\n\\[\n\\dfrac{\\partial u}{\\partial x} = X'Y, \\quad\n\\dfrac{\\partial u}{\\partial y} = XY', \\quad\n\\dfrac{\\partial^2 u}{\\partial x^2} = X''Y, \\quad\n\\dfrac{\\partial^2 u}{\\partial y^2} = XY'',\n\\]\ndonde las comillas (primes) denotan derivación ordinaria.",
    "crumbs": [
      "Preliminares",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Ecuaciones diferenciales parciales</span>"
    ]
  },
  {
    "objectID": "partial_diff_eq.html#principio-de-superposición",
    "href": "partial_diff_eq.html#principio-de-superposición",
    "title": "2  Ecuaciones diferenciales parciales",
    "section": "2.4 Principio de superposición",
    "text": "2.4 Principio de superposición\n\nTeorema 2.1 Si \\(u_1 , u_2 , \\dots , u_k\\) son soluciones de una ecuación diferencial parcial lineal homogénea, entonces la combinación lineal \\[\nu = c_1u_1 + c_2u_2 + \\dots + c_ku_k\n\\] donde las \\(c_1=1,2,\\dots,k\\) son constantes. Es también una solución.\n\nEl teorema 2.1 se puede entender como: siempre que tengamos un conjunto infinito de soluciones \\(u_1, u_2, u_3, \\ldots\\) de una ecuación lineal homogénea, podemos construir otra solución \\(u\\) mediante la serie infinita:\n\\[  \nu = \\sum_{k=1}^{\\infty} c_k u_k,  \n\\]\ndonde las constantes \\(c_i\\), con \\(i = 1, 2, \\ldots\\), son coeficientes.",
    "crumbs": [
      "Preliminares",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Ecuaciones diferenciales parciales</span>"
    ]
  },
  {
    "objectID": "partial_diff_eq.html#clasificación-de-ecuaciones",
    "href": "partial_diff_eq.html#clasificación-de-ecuaciones",
    "title": "2  Ecuaciones diferenciales parciales",
    "section": "2.5 Clasificación de ecuaciones",
    "text": "2.5 Clasificación de ecuaciones\nUna ecuación diferencial parcial lineal de segundo orden con dos variables independientes y coeficientes constantes puede clasificarse en uno de tres tipos. Esta clasificación depende únicamente de los coeficientes de las derivadas de segundo orden. Por supuesto, asumimos que al menos uno de los coeficientes \\(A\\), \\(B\\) o \\(C\\) es distinto de cero.\n\nDefinición 2.1 La ecuación diferencial parcial lineal de segundo orden \\[\nA \\dfrac{\\partial^2 u}{\\partial x^2} + B \\dfrac{\\partial^2 u}{\\partial x \\, \\partial y} + C \\dfrac{\\partial^2 u}{\\partial y^2} + D \\dfrac{\\partial u}{\\partial x} + E \\dfrac{\\partial u}{\\partial y} + F u = 0,\n\\] donde \\(A,B,C,D,F\\) son constantes reales, se dice que es:\n\nHiperbólica si \\(\\quad B^2-4AC&gt;0\\),\nParabólica si \\(\\quad B^2-4AC=0\\),\nElíptica si \\(\\quad B^2-4AC&lt;0\\).",
    "crumbs": [
      "Preliminares",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Ecuaciones diferenciales parciales</span>"
    ]
  },
  {
    "objectID": "boundary_problems.html#ecuaciones-clásicas",
    "href": "boundary_problems.html#ecuaciones-clásicas",
    "title": "3  Problemas de valores en la frontera",
    "section": "3.1 Ecuaciones clásicas",
    "text": "3.1 Ecuaciones clásicas\nAplicar el método de separación de variables para encontrar soluciones en forma de producto es muy común con las siguientes ecuaciones clásicas de la física matemática:\n\\[  \nk\\frac{\\partial^2 u}{\\partial x^2} = \\frac{\\partial u}{\\partial t}, \\quad k &gt; 0  \n\\tag{3.1}\\]\n\\[  \n\\alpha^2 \\frac{\\partial^2 u}{\\partial x^2} = \\frac{\\partial^2 u}{\\partial t^2}  \n\\tag{3.2}\\]\n\\[  \n\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = 0  \n\\tag{3.3}\\]\no variantes ligeras de estas ecuaciones. Las EDPs 3.1, 3.2 y 3.3 se conocen, respectivamente, como la ecuación del calor unidimensional, la ecuación de onda unidimensional y la forma bidimensional de la ecuación de Laplace. El término “unidimensional” en el caso de las ecuaciones 3.1 y 3.2 se refiere al hecho de que \\(x\\) denota una variable espacial, mientras que \\(t\\) representa el tiempo; “bidimensional” en 3.3 significa que tanto \\(x\\) como \\(y\\) son variables espaciales. Si comparas 3.1-3.3 con la forma lineal en la Definición 2.1 (donde \\(t\\) juega el papel del símbolo \\(y\\)), observarás que la ecuación del calor 3.1 es parabólica, la ecuación de onda 3.2 es hiperbólica y la ecuación de Laplace 3.3 es elíptica.\n\n\n\n\n\n\n\n\n\n\n\n(a) Flujo de calor unidimensional.\n\n\n\n\n\n\n\n\n\n\n\n(b) Cuerda tensada.\n\n\n\n\n\n\n\nFigura 3.1: Aplicaciones de las ecuaciones 3.1 y 3.2 (Zill y Cullen 2008).",
    "crumbs": [
      "Preliminares",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Problemas de valores en la frontera</span>"
    ]
  },
  {
    "objectID": "boundary_problems.html#condiciones-iniciales",
    "href": "boundary_problems.html#condiciones-iniciales",
    "title": "3  Problemas de valores en la frontera",
    "section": "3.2 Condiciones iniciales",
    "text": "3.2 Condiciones iniciales\nDado que las soluciones de las ecuaciones 3.1 y 3.2 dependen del tiempo \\(t\\), es posible especificar lo que ocurre en \\(t = 0\\); es decir, establecer condiciones iniciales (CI). Si \\(f(x)\\) representa la distribución inicial de temperatura en la varilla mostrada en la Figura 3.1 (a), entonces una solución \\(u(x, t)\\) de 3.1 debe satisfacer la condición inicial única \\(u(x, 0) = f(x), \\quad 0 &lt; x &lt; L\\).\nPor otro lado, para una cuerda vibrante podemos especificar tanto su desplazamiento inicial (o forma) \\(f(x)\\) como su velocidad inicial \\(g(x)\\). En términos matemáticos, buscamos una función \\(u(x, t)\\) que satisfaga 3.2 y las dos condiciones iniciales: \\[\nu(x, 0) = f(x), \\quad \\left. \\frac{\\partial u}{\\partial t} \\right|_{t=0} = g(x), \\quad 0 &lt; x &lt; L.\n\\tag{3.4}\\]\nPor ejemplo, la cuerda podría ser tensada, como se muestra en la Figura 3.1 (b), y liberada desde el reposo \\((g(x)=0)\\).",
    "crumbs": [
      "Preliminares",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Problemas de valores en la frontera</span>"
    ]
  },
  {
    "objectID": "boundary_problems.html#condiciones-de-frontera",
    "href": "boundary_problems.html#condiciones-de-frontera",
    "title": "3  Problemas de valores en la frontera",
    "section": "3.3 Condiciones de frontera",
    "text": "3.3 Condiciones de frontera\nLa cuerda en la Figura 3.1 (b) está fija al eje \\(x\\) en \\(x=0\\) y \\(x=L\\) para todos los tiempos. Ésto se interpreta a través de dos condiciones de frontera (CF): \\[\nu(0,t) = 0, \\quad u(L,t)=0, \\quad t&gt;0.\n\\] En éste contexto la función \\(f\\) en la ec 3.4 es continua y, en consecuencia, \\(f(0)=0\\) y \\(f(L)=0\\). En general, existen tres tipos de condicones de frontera asociadas con las ecuaciones 3.1, 3.2 y 3.3. En la frontera es posible especificar los valores de una de las siguientes:\n\\[\n\\text{(i)}\\quad u, \\qquad \\text{(ii)}\\quad \\dfrac{\\partial u}{\\partial n},\\qquad \\text{or}\\qquad \\text{(iii)}\\quad \\dfrac{\\partial u}{\\partial n} + hu,\\quad \\text{con $h$ constante.}\n\\]\nAquí \\(\\frac{\\partial u}{\\partial n}\\) denota la derivada normal de \\(u\\) (la derivada de \\(u\\) en dirección perpendicular a la frontera). Una condición de frontera del primer tipo (i) es llamada condición de Dirichlet; una condición de frontera del segundo tipo (ii) es llamada condición de Neumann; y una condición de frontera del tercer tipo (iii) es conocida como condición de Robin. Por ejemplo, para \\(t&gt;0\\) una condición típica al extremo derecho de la varilla de la Figura 3.1 (a) puede ser:\n\\[\\begin{align*}\n  \\text{(i)}'   &\\quad u(L,t) = u_0, \\text{  con } u_0 \\text{ constante} \\\\ \\\\\n  \\text{(ii)}'  &\\quad \\left. \\dfrac{\\partial u}{\\partial x} \\right|_{x=L} = 0 \\\\ \\\\\n  \\text{(iii)}' &\\quad \\left. \\dfrac{\\partial u}{\\partial x} \\right|_{x=L} = -h(u(L,t)-u_m),\\text{  con } h&gt;0 \\text{ y } u_m \\text{ constantes}\n\\end{align*}\\]\nLa condición (i)’ simplemente establece que el límite \\(x=L\\) se mantiene, por algún medio, a una temperatura constante \\(u_0\\) durante todo el tiempo \\(t&gt;0\\). La condición (ii)’ indica que el contorno \\(x=L\\) está aislado. Según la ley empírica de la transferencia de calor, el flujo de calor a través del borde (es decir, la cantidad de calor por unidad de área por unidad de tiempo conducida a través la frontera) es proporcional al valor de la derivada normal \\(\\frac{\\partial u}{\\partial n}\\) de la temperatura \\(u\\). Por lo tanto, cuando el límite \\(x=L\\) está aislado térmicamente, no fluye calor hacia dentro ni hacia fuera de la varilla, por lo que \\[\n\\left. \\dfrac{\\partial u}{\\partial x} \\right|_{x=L} = 0.\n\\]\nEs posible interpretar (iii)’ como que el calor se pierde del extremo derecho de la varilla al estar en contacto con un medio, como el aire o el agua, que se mantiene a temperatura constante. Según la ley de enfriamiento de Newton, el flujo de calor hacia afuera de la varilla es proporcional a la diferencia entre la temperatura \\(u(L, t)\\) en la frontera y la temperatura \\(u_m\\) del medio circundante. Se observa que si se pierde calor por el extremo izquierdo de la varilla, la condición de contorno es \\[\n\\left. \\dfrac{\\partial u}{\\partial x} \\right|_{x=0} = h(u(0,t)-u_m).\n\\] El cambio de signo respecto de (iii)’ corresponde con el supuesto de que la varilla está a una temperatura más alta que el medio que rodea los extremos, de modo que \\(u(0, t) &gt; u_m\\) y \\(u(L, t) &gt; u_m\\). Para \\(x=0\\) y \\(x=L\\), las pendientes \\(u_x(0, t)\\) y \\(u_x(L, t)\\) deben ser positivas y negativas, respectivamente.\nPor supuesto, en los extremos de la varilla se pueden especificar diferentes condiciones al mismo tiempo. Por ejemplo, podríamos tener \\[\n\\left. \\dfrac{\\partial u}{\\partial x} \\right|_{x=0} =0 \\quad \\text{y} \\qquad u(L,t)=u_0, \\quad t&gt;0.\n\\]",
    "crumbs": [
      "Preliminares",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Problemas de valores en la frontera</span>"
    ]
  },
  {
    "objectID": "initial_values.html#problemas-bien-plantedos",
    "href": "initial_values.html#problemas-bien-plantedos",
    "title": "4  Problemas de valor inicial",
    "section": "4.1 Problemas bien plantedos",
    "text": "4.1 Problemas bien plantedos\nAhora que hemos abordado, hasta cierto punto, la cuestión de cuándo los problemas de valor inicial tienen soluciones únicas, podemos pasar a la segunda consideración importante: cuándo aproximar la solución de un problema de valor inicial. Los problemas de valor inicial obtenidos mediante la observación de fenómenos físicos generalmente solo se aproximan a la situación real, por lo que necesitamos saber si pequeños cambios en el plamteamiento del problema introducen cambios correspondientemente pequeños en la solución (Burden y Faires 2010).\nA continiación se presentan otras definiciones así como teoremas que brindaŕan un conocimiento más sólido acerca de los problemas bien planteados. Se usará como referencia a Burden y Faires (2010).\n\nDefinición 4.3 Se dice que el problema de valor inicial \\[\n\\dfrac{dy}{dt} = f(t,y), \\quad a\\leq t \\leq b, \\quad y(a) = \\alpha,\n\\tag{4.2}\\] es un problema bien planteado si:\n\nExiste una única solución \\(y(t)\\) para el problema, y\nExisten constantes \\(ε_0 &gt; 0\\) y \\(k &gt; 0\\) tales que para cualquier \\(\\varepsilon\\), con \\(\\varepsilon_0 &gt; \\varepsilon &gt; 0\\), siempre que \\(δ(t)\\) sea continua con \\(|δ(t)| &lt; \\varepsilon\\) para todo \\(t\\) en \\([a, b]\\), y cuando \\(|δ_0| &lt; \\varepsilon\\), el problema del valor inicial \\[\n\\dfrac{dz}{dt} = f(t,z) + δ(t), \\quad a\\leq t \\leq b, \\quad z(a) = \\alpha + δ_0\n\\tag{4.3}\\] tenga una única solución \\(z(t)\\) que satisface: \\[\n|z(t)-y(t)| &lt; k\\varepsilon \\quad \\forall t \\in[a,b]\n\\]\n\n\nEl problema especificado por al Ecuación 4.3 se denomina problema perturbado asociado al problema original Ecuación 4.2. Se asume la posibilidad de que se introduzca un error en el planteamiento de la ecuación diferencial, así como la presencia de un error \\(δ_0\\) en la condición inicial.\nLos métodos numéricos siempre se centrarán en la solución de un problema perturbado, ya que cualquier error de redondeo introducido en la representación perturba el problema original. A menos que el problema original esté bien planteado, hay pocas razones para esperar que la solución numérica de un problema perturbado se aproxime con precisión a la solución del problema original.\n\nTeorema 4.3 Supongamos \\(D = \\{(t, y) | \\ \\ a \\leq t \\leq b, \\ \\  -\\infty &lt; y &lt; \\infty \\}\\). Si \\(f\\) es continua y satisface una condición de Lipschitz en la variable \\(y\\) en el conjunto \\(D\\), entonces el problema de valor inicial \\[\n\\dfrac{dy}{dt} = f(t,y), \\quad a\\leq t \\leq b, \\quad y(a) = \\alpha\n\\] es bien planteado.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nDemostrar que el problema de valor inicial\n\\[\n\\frac{dy}{dt} = y - t^2 + 1, \\quad 0 \\leq t \\leq 2, \\quad y(0) = 0.5,\n\\]\nestá bien planteado en el dominio \\(D = \\{(t, y) \\mid \\ \\ 0 \\leq t \\leq 2 \\ \\ \\text{ y } \\ \\ -\\infty &lt; y &lt; \\infty \\}\\).\nSolución: Dado que\n\\[\n\\left| \\frac{\\partial (y - t^2 + 1)}{\\partial y} \\right| = |1| = 1,\n\\tag{4.4}\\]\nel Teorema 4.1 implica que la función \\(f(t, y) = y - t^2 + 1\\) satisface una condición de Lipschitz en \\(y\\) sobre \\(D\\) con constante de Lipschitz igual a 1. Además, como \\(f\\) es continua en \\(D\\), el Teorema 4.3 garantiza que el problema está bien planteado.\nA modo de ilustración, consideremos ahora la solución del problema perturbado:\n\\[\n\\frac{dz}{dt} = z - t^2 + 1 + \\delta, \\quad 0 \\leq t \\leq 2, \\quad z(0) = 0.5 + \\delta_0,\n\\tag{4.5}\\]\ndonde \\(\\delta\\) y \\(\\delta_0\\) son constantes pequeñas, las soluciones respectivas de las ecuaciones 4.4 y 4.5 son:\n\\[\\begin{align*}\ny(t) &= (t + 1)^2 - 0.5e^t \\\\ \\\\\nz(t) &= (t + 1)^2 + (\\delta + \\delta_0 - 0.5)e^t - \\delta\n\\end{align*}\\]\nSea \\(\\varepsilon\\) un número positivo. Si \\(|\\delta| &lt; \\varepsilon\\) y \\(|\\delta_0| &lt; \\varepsilon\\), entonces \\[\n|y(t) - z(t)| = |(\\delta + \\delta_0)e^t - \\delta| \\leq |\\delta + \\delta_0|e^2 + |\\delta| \\leq (2e^2 + 1)\\varepsilon,\n\\]\npara todo \\(t\\). Esta desigualdad demuestra que 4.4 está bien planteado, con una constante de estabilidad \\(k(\\varepsilon) = 2e^2 + 1\\) para cualquier \\(\\varepsilon &gt; 0\\).",
    "crumbs": [
      "Preliminares",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Problemas de valor inicial</span>"
    ]
  },
  {
    "objectID": "texto_crank_nick.html",
    "href": "texto_crank_nick.html",
    "title": "5  Método de Crank Nickolson",
    "section": "",
    "text": "Existen métodos implícitos de diferencias finitas para resolver ecuaciones diferenciales parciales parabólicas ec 3.1. Estos métodos requieren la resolución de un sistema de ecuaciones para determinar los valores aproximados de \\(u\\) en la línea de tiempo \\((j + 1)\\). Sin embargo, los métodos implícitos no presentan problemas de inestabilidad (Zill y Cullen 2008).\nEl algoritmo introducido por J. Crank y P. Nicholson en 1947 se utiliza principalmente para resolver la ecuación del calor. El algoritmo consiste en sustituir la segunda derivada parcial en \\(c\\frac{\\partial^2 u}{\\partial x^2} = \\frac{\\partial u}{\\partial t}\\) por el promedio de dos cocientes de diferencias centrales, uno evaluado en \\(t\\) y el otro en \\(t+k\\): \\[\n\\begin{split}\n\\dfrac{c}{2}& \\left[\\dfrac{u(x+h,t)-2u(x,t)+u(x-h,t)}{h^2}\\right] + \\\\\n    + &\\dfrac{c}{2} \\left[\\dfrac{u(x+h,t+k)-2u(x,t+k)+u(x-h,t+k)}{h^2} \\right] \\\\\n            &= \\dfrac{1}{k}[u(x,t+k)-u(x,t)]\n\\end{split}\n\\tag{5.1}\\]\nsi se define \\(\\lambda = \\frac{ck}{h^2}\\) y \\[\n\\begin{split}\nu(x+h,t) &=u_{i+1,j}, \\quad u(x,t)=u_{ij}, \\quad u(x-h,t)=u_{i-1,j}, \\\\\nu(x+h,t+k) &=u_{i+1,j+1}, \\quad u(x,t+k)=u_{i.j+1}, \\quad u(x-h,t+k)=u_{i-1,j+1},\n\\end{split}\n\\]\nes posible reescribir a la eq 5.1 como: \\[\n-u_{i-1,j+1} + \\alpha u_{i,j+1} - u_{i+1,j+1} = u_{i+1,j} - \\beta u_{ij} + u_{i-1,j},\n\\tag{5.2}\\]\ndonde \\(\\alpha=2(1+\\frac{1}{\\lambda})\\), \\(\\beta=2(1-\\frac{1}{\\lambda})\\), \\(j=0,1,\\dots m-1 \\ \\ \\ \\text{e} \\ \\ \\ i=0,1,\\dots n-1\\).\nPara cada elección de \\(j\\) la ecuación diferencial eq 5.2 para \\(i=0,1,\\dots n-1\\) da \\(n-1\\) ecuaciones en \\(n-1\\) incógnitas \\(u_{i,j+1}\\). Debido a las condiciones de contorno preestablecidas, los valores de \\(u_{i, j+1}\\) se conocen para \\(i=0\\) y para \\(i=n\\). Por ejemplo, en el caso \\(n=4\\), el sistema de ecuaciones para determinar los valores aproximados de \\(u\\) en la línea de tiempo \\((j+1)\\) es:\n\\[\n\\begin{split}\n-u_{0,j+1} + \\alpha u_{1,j+1} - u_{2,j+1} =& u_{2,j} - \\beta u_{1,j} + u_{0,j} \\\\\n-u_{1,j+1} + \\alpha u_{2,j+1} - u_{3,j+1} =& u_{3,j} - \\beta u_{2,j} + u_{1,j} \\\\\n-u_{2,j+1} + \\alpha u_{3,j+1} - u_{4,j+1} =& u_{4,j} - \\beta u_{3,j} + u_{2,j}\n\\end{split}\n\\]\nreordenando se llega a \\[\n\\begin{matrix}\n\\alpha u_{1,j+1} &-& u_{2,j+1}&& &= b_1 \\\\\n-u_{1,j+1} &+& \\alpha u_{2,j+1} &-& u_{3,j+1} &= b_2 \\\\\n&-&u_{2,j+1} &+& \\alpha u_{3,j+1} &= b_3\n\\end{matrix}\n\\tag{5.3}\\]\ndonde \\[\\begin{align*}\nb_1 &= u_{2,j} - \\beta u_{1,j} + u_{0,j} + u_{0,j+1}, \\\\\nb_2 &= u_{3,j} - \\beta u_{2,j} + u_{1,j}, \\\\\nb_3 &= u_{4,j} - \\beta u_{3,j} + u_{2,j} + u_{4,j+1}.\n\\end{align*}\\]\nEn general, si utilizamos la ecuación diferencial eq 5.2 para determinar valores de \\(u\\) en la línea de tiempo \\((j -1)\\), es necesario resolver un sistema lineal \\(\\mathbf{AX=B}\\), donde la matriz de coeficientes \\(\\mathbf{A}\\) es una matriz tridiagonal, \\[\nA =\n\\begin{pmatrix}\n\\alpha & -1    & 0      & 0      & 0      & \\cdots & 0      \\\\\n-1     & \\alpha & -1     & 0      & 0      & \\cdots & 0      \\\\\n0      & -1     & \\alpha & -1     & 0      & \\cdots & 0      \\\\\n0      & 0      & -1     & \\alpha & -1     & \\cdots & 0      \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0      & 0      & 0      & 0      & 0      & \\cdots & \\alpha & -1 \\\\\n0      & 0      & 0      & 0      & 0      & \\cdots & -1     & \\alpha\n\\end{pmatrix},\n\\]\ny las componentes de la matriz columna \\(\\mathbf{B}\\) son\n\\[\\begin{align*}\nb_1 &= u_{2,j} - \\beta u_{1,j} + u_{0,j} + u_{0,j+1}, \\\\\nb_2 &= u_{3,j} - \\beta u_{2,j} + u_{1,j}, \\\\\nb_3 &= u_{4,j} - \\beta u_{3,j} + u_{2,j}, \\\\\n    &\\vdots \\\\\nb_{n-1} &= u_{n,j} - \\beta u_{n-1,j} + u_{n-2,j} + u_{n,j+1}.\n\\end{align*}\\]\n\n\n\n\n\nZill, Dennis G., y Michael R. Cullen. 2008. «Differential Equations with Boundary-Value Problems». En, 7.ª ed., 433-42. Belmont, CA: Cengage Learning.",
    "crumbs": [
      "Preliminares",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Método de Crank Nickolson</span>"
    ]
  },
  {
    "objectID": "pinns.html#ejemplo-de-resolución-de-la-ecuación-de-burgers-1d-con-deepxde",
    "href": "pinns.html#ejemplo-de-resolución-de-la-ecuación-de-burgers-1d-con-deepxde",
    "title": "6  Physic Informed Neural Networks (PINNs)",
    "section": "6.1 Ejemplo de resolución de la ecuación de Burger’s 1D con deepxde",
    "text": "6.1 Ejemplo de resolución de la ecuación de Burger’s 1D con deepxde\nDada la ecuación: \\[\n\\dfrac{\\partial u}{\\partial t} + u\\dfrac{\\partial u}{\\partial x} = v\\dfrac{\\partial^2 u}{\\partial x^2} \\quad, x \\in[-1,1], \\ t\\in[0,1]\n\\]\ncon la condición de frontera de Dirichlet y condición inicial: \\[\nu(-1,t) = u(1,t) = 0, \\quad u(x,0)=-\\sin(\\pi x)\n\\]\n\n\nCódigo\nimport deepxde as dde\nimport numpy as np\n\n\ndef gen_testdata():\n    data = np.load(\"data/Burgers.npz\")\n    t, x, exact = data[\"t\"], data[\"x\"], data[\"usol\"].T\n    xx, tt = np.meshgrid(x, t)\n    X = np.vstack((np.ravel(xx), np.ravel(tt))).T\n    y = exact.flatten()[:, None]\n    return X, y\n\n\ndef pde(x, y):\n    dy_x = dde.grad.jacobian(y, x, i=0, j=0)\n    dy_t = dde.grad.jacobian(y, x, i=0, j=1)\n    dy_xx = dde.grad.hessian(y, x, i=0, j=0)\n    return dy_t + y * dy_x - 0.01 / np.pi * dy_xx\n\n\ngeom = dde.geometry.Interval(-1, 1)\ntimedomain = dde.geometry.TimeDomain(0, 0.99)\ngeomtime = dde.geometry.GeometryXTime(geom, timedomain)\n\nbc = dde.icbc.DirichletBC(geomtime, lambda x: 0, lambda _, on_boundary: on_boundary)\nic = dde.icbc.IC(\n    geomtime, lambda x: -np.sin(np.pi * x[:, 0:1]), lambda _, on_initial: on_initial\n)\n\ndata = dde.data.TimePDE(\n    geomtime, pde, [bc, ic],\n    num_domain=2540,\n    num_boundary=80,\n    num_initial=160,\n    num_test=300\n)\nnet = dde.nn.FNN([2] + [20] * 3 + [1], \"tanh\", \"Glorot normal\")\nmodel = dde.Model(data, net)\n\nmodel.compile(\"adam\", lr=1e-3)\nmodel.train(iterations=5000)\nmodel.compile(\"L-BFGS\")\nlosshistory, train_state = model.train()\ndde.saveplot(losshistory, train_state, issave=False, isplot=True)\n\nX, y_true = gen_testdata()\ny_pred = model.predict(X)\nf = model.predict(X, operator=pde)\nprint(\"Mean residual:\", np.mean(np.absolute(f)))\nprint(\"L2 relative error:\", dde.metrics.l2_relative_error(y_true, y_pred))\n\n\n2025-06-25 15:38:49.026838: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2025-06-25 15:38:49.080927: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2025-06-25 15:38:49.081802: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-06-25 15:38:50.022280: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nUsing backend: tensorflow.compat.v1\nOther supported backends: tensorflow, pytorch, jax, paddle.\npaddle supports more examples now and is recommended.\n\n\nWARNING:tensorflow:From /home/damian/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\nInstructions for updating:\nnon-resource variables are not supported in the long term\nCompiling model...\nBuilding feed-forward neural network...\n'build' took 0.044881 s\n\n\n\n/home/damian/.local/lib/python3.8/site-packages/deepxde/nn/tensorflow_compat_v1/fnn.py:116: UserWarning:\n\n`tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n\n\n\n'compile' took 0.349554 s\n\nTraining model...\n\nStep      Train loss                        Test loss                         Test metric\n0         [9.90e-03, 5.31e-03, 5.32e-01]    [9.53e-03, 5.31e-03, 5.32e-01]    []  \n\n\n2025-06-25 15:38:51.672960: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n\n\n1000      [3.91e-02, 4.68e-04, 5.24e-02]    [4.39e-02, 4.68e-04, 5.24e-02]    []  \n2000      [2.28e-02, 7.61e-05, 3.03e-02]    [5.67e-02, 7.61e-05, 3.03e-02]    []  \n3000      [1.05e-02, 4.22e-05, 6.66e-03]    [6.38e-02, 4.22e-05, 6.66e-03]    []  \n4000      [4.69e-03, 2.22e-05, 2.92e-03]    [3.82e-02, 2.22e-05, 2.92e-03]    []  \n5000      [3.82e-03, 1.46e-05, 2.02e-03]    [2.67e-02, 1.46e-05, 2.02e-03]    []  \n\nBest model at step 5000:\n  train loss: 5.86e-03\n  test loss: 2.88e-02\n  test metric: []\n\n'train' took 38.613541 s\n\nCompiling model...\n'compile' took 0.191548 s\n\nTraining model...\n\nStep      Train loss                        Test loss                         Test metric\n5000      [3.82e-03, 1.46e-05, 2.02e-03]    [2.67e-02, 1.46e-05, 2.02e-03]    []  \n6000      [2.79e-04, 6.99e-07, 1.37e-04]    [2.79e-04, 6.99e-07, 1.37e-04]        \n7000      [9.52e-05, 3.08e-07, 5.34e-05]    [9.52e-05, 3.08e-07, 5.34e-05]        \n8000      [3.84e-05, 1.15e-07, 1.07e-05]    [3.84e-05, 1.15e-07, 1.07e-05]        \n9000      [1.95e-05, 4.68e-08, 4.82e-06]    [1.95e-05, 4.68e-08, 4.82e-06]        \n10000     [1.15e-05, 3.38e-08, 2.46e-06]    [1.15e-05, 3.38e-08, 2.46e-06]        \nINFO:tensorflow:Optimization terminated with:\n  Message: CONVERGENCE: REL_REDUCTION_OF_F_&lt;=_FACTR*EPSMCH\n  Objective function value: 0.000014\n  Number of iterations: 4792\n  Number of functions evaluations: 5097\n10097     [1.14e-05, 4.10e-08, 2.28e-06]    [6.58e-03, 4.10e-08, 2.28e-06]    []  \n\nBest model at step 10097:\n  train loss: 1.37e-05\n  test loss: 6.58e-03\n  test metric: []\n\n'train' took 55.594739 s\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean residual: 0.0037696785\nL2 relative error: 0.0037413991822282254",
    "crumbs": [
      "Redes neuronales",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Physic Informed Neural Networks (PINNs)</span>"
    ]
  },
  {
    "objectID": "pinns.html#comparación-con-redes-neuronales-tradicionales",
    "href": "pinns.html#comparación-con-redes-neuronales-tradicionales",
    "title": "6  Physic Informed Neural Networks (PINNs)",
    "section": "6.2 Comparación con Redes Neuronales Tradicionales",
    "text": "6.2 Comparación con Redes Neuronales Tradicionales\nMientras que las redes neuronales tradicionales dependen exclusivamente de grandes volúmenes de datos etiquetados para su entrenamiento (Karniadakis et al. 2021), las PINNs integran el conocimiento físico como parte esencial de su arquitectura (Blechschmidt y Ernst 2021). Esta diferencia clave permite a las PINNs generar soluciones físicamente consistentes incluso con datos escasos, evitando el sobreajuste común en enfoques puramente basados en datos. Otra ventaja significativa de las PINNs es su naturaleza mesh-free, que contrasta con los métodos numéricos tradicionales como FEM o FDM que requieren discretización espacial. Sin embargo, el entrenamiento de PINNs puede ser más desafiante debido a la necesidad de optimizar múltiples objetivos simultáneamente (ajuste a datos y cumplimiento de leyes físicas) (Blechschmidt y Ernst 2021; Karniadakis et al. 2021).",
    "crumbs": [
      "Redes neuronales",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Physic Informed Neural Networks (PINNs)</span>"
    ]
  },
  {
    "objectID": "deeponet.html#arquitectura",
    "href": "deeponet.html#arquitectura",
    "title": "7  DeepONet",
    "section": "7.1 Arquitectura",
    "text": "7.1 Arquitectura\nLa arquitectura de DeepONet está compuesta por dos redes principales: la red de branch y la red de trunk. La red branch procesa las evaluaciones discretas de la función de entrada (por ejemplo, condiciones iniciales o de frontera), mientras que la red trunk recibe como entrada los puntos del dominio donde se desea evaluar la función de salida. La salida final se obtiene mediante el producto punto de los vectores generados por ambas redes, lo que permite representar operadores complejos con alta generalización a nuevos datos (Lu et al. 2021).\n\n\n\n\n\n\nFigura 7.1: Ilustraciones del planteamiento del problema y arquitectura DeepONet que conducen a una buena generalización. a) Para que la red aprenda un operador \\(G : u \\rightarrow G(u)\\) se necesitan dos entradas \\([u(x_1), u(x_2), ..., u(x_m)]\\) e \\(y\\). b) Ilustración de los datos de entrenamiento. Para cada función de entrada \\(u\\), se requiere el mismo número de evaluaciones en los mismos sensores dispersos \\(x_1, x_2, ..., x_m\\). Sin embargo, no se impone ninguna restricción sobre el número ni las ubicaciones para la evaluación de las funciones de salida. c) La DeepONet stacked se inspira en el Teorema de aproximación universal para operadores y consta de una red Trunk y \\(p\\) redes Branch apiladas. La red cuya construcción se inspira en el mismo teorema es una DeepONet stacked formada al elegir la red Trunk como una red de una capa de ancho \\(p\\) y cada red Branch como una red de una capa oculta de ancho \\(n\\). d) La red DeepONet unstacked se inspira en el Teorema general de aproximación universal para operadores y consta de una red Trunk y una red Branch. Una red DeepONet unstacked puede considerarse como una red DeepONet stacked, en la que todas las redes Branch comparten el mismo conjunto de parámetros (Lu et al. 2021).",
    "crumbs": [
      "Redes neuronales",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>DeepONet</span>"
    ]
  },
  {
    "objectID": "deeponet.html#ejemplo-de-resolución-de-un-operador-usando-deeponet",
    "href": "deeponet.html#ejemplo-de-resolución-de-un-operador-usando-deeponet",
    "title": "7  DeepONet",
    "section": "7.2 Ejemplo de resolución de un operador usando DeepONet",
    "text": "7.2 Ejemplo de resolución de un operador usando DeepONet\nSe resolverá el operador \\[\nG: f\\rightarrow u\n\\]\npara el problema unidemensional de Poisson: \\[\nu''(x) = f(x), \\quad x\\in[0,1]\n\\]\ncon la condición de frontera de Dirichlet \\[\nu(0)=u(1)=0\n\\]\nel término \\(f\\) representa a una función continua arbitraria.\n\n\nCódigo\nimport deepxde as dde\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# Poisson equation: -u_xx = f\ndef equation(x, y, f):\n    dy_xx = dde.grad.hessian(y, x)\n    return -dy_xx - f\n\n# Domain is interval [0, 1]\ngeom = dde.geometry.Interval(0, 1)\n\n# Zero Dirichlet BC\ndef u_boundary(_):\n    return 0\n\ndef boundary(_, on_boundary):\n    return on_boundary\n\nbc = dde.icbc.DirichletBC(geom, u_boundary, boundary)\n\n# Define PDE\npde = dde.data.PDE(geom, equation, bc, num_domain=100, num_boundary=2)\n\n# Function space for f(x) are polynomials\ndegree = 3\nspace = dde.data.PowerSeries(N=degree + 1)\n\n# Choose evaluation points\nnum_eval_points = 10\nevaluation_points = geom.uniform_points(num_eval_points, boundary=True)\n\n# Define PDE operator\npde_op = dde.data.PDEOperatorCartesianProd(\n    pde,\n    space,\n    evaluation_points,\n    num_function=100,\n    num_test=20\n)\n\n# Setup DeepONet\ndim_x = 1\np = 32\nnet = dde.nn.DeepONetCartesianProd(\n    [num_eval_points, 32, p],\n    [dim_x, 32, p],\n    activation=\"tanh\",\n    kernel_initializer=\"Glorot normal\",\n)\n\n# Define and train model\nmodel = dde.Model(pde_op, net)\ndde.optimizers.set_LBFGS_options(maxiter=1000)\nmodel.compile(\"L-BFGS\")\nmodel.train()\n\n# Plot realisations of f(x)\nn = 3\nfeatures = space.random(n)\nfx = space.eval_batch(features, evaluation_points)\n\nx = geom.uniform_points(100, boundary=True)\ny = model.predict((fx, x))\n\n# Setup figure\nfig = plt.figure(figsize=(7, 8))\nplt.subplot(2, 1, 1)\nplt.title(\"Ecuación de Poisson: término f(x) y solución u(x)\")\nplt.ylabel(\"f(x)\")\nz = np.zeros_like(x)\nplt.plot(x, z, \"k-\", alpha=0.1)\n\n# Plot source term f(x)\nfor i in range(n):\n    plt.plot(evaluation_points, fx[i], \"--\")\n\n# Plot solution u(x)\nplt.subplot(2, 1, 2)\nplt.ylabel(\"u(x)\")\nplt.plot(x, z, \"k-\", alpha=0.1)\nfor i in range(n):\n    plt.plot(x, y[i], \"-\")\nplt.xlabel(\"x\")\n\nplt.show()\n\n\n2025-06-25 15:44:51.126319: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2025-06-25 15:44:51.182926: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2025-06-25 15:44:51.184076: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2025-06-25 15:44:52.307785: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\nUsing backend: tensorflow.compat.v1\nOther supported backends: tensorflow, pytorch, jax, paddle.\npaddle supports more examples now and is recommended.\n\n\nWARNING:tensorflow:From /home/damian/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\nInstructions for updating:\nnon-resource variables are not supported in the long term\nCompiling model...\nBuilding DeepONetCartesianProd...\n'build' took 0.072817 s\n\n\n\n/home/damian/.local/lib/python3.8/site-packages/deepxde/nn/tensorflow_compat_v1/deeponet.py:549: UserWarning:\n\n`tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n\n/home/damian/.local/lib/python3.8/site-packages/deepxde/nn/tensorflow_compat_v1/deeponet.py:556: UserWarning:\n\n`tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n\n/home/damian/.local/lib/python3.8/site-packages/deepxde/nn/tensorflow_compat_v1/deeponet.py:570: UserWarning:\n\n`tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n\n\n\n'compile' took 32.253996 s\n\n\n\n2025-06-25 15:45:26.131743: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n\n\nTraining model...\n\nStep      Train loss              Test loss               Test metric\n0         [1.10e+00, 7.24e-01]    [7.99e-01, 5.92e-01]    []  \n1000      [5.46e-06, 3.84e-07]    [6.38e-06, 3.50e-07]        \nINFO:tensorflow:Optimization terminated with:\n  Message: STOP: TOTAL NO. of ITERATIONS REACHED LIMIT\n  Objective function value: 0.000004\n  Number of iterations: 1000\n  Number of functions evaluations: 1066\n1066      [4.03e-06, 2.17e-07]    [4.07e-06, 1.75e-07]    []  \n\nBest model at step 1066:\n  train loss: 4.25e-06\n  test loss: 4.25e-06\n  test metric: []\n\n'train' took 34.982487 s",
    "crumbs": [
      "Redes neuronales",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>DeepONet</span>"
    ]
  },
  {
    "objectID": "deeponet.html#comparación-con-una-pinn",
    "href": "deeponet.html#comparación-con-una-pinn",
    "title": "7  DeepONet",
    "section": "7.3 Comparación con una PINN",
    "text": "7.3 Comparación con una PINN\nEn contraste con una red PINN convencional (Physics-Informed Neural Network), que resuelve una instancia específica de una ecuación diferencial para un conjunto dado de condiciones, DeepONet aprende el operador general que resuelve muchas instancias a la vez. Mientras que una PINN debe ser reentrenada para cada nuevo problema, DeepONet, una vez entrenado, puede predecir soluciones rápidamente para múltiples condiciones nuevas. Esto lo hace especialmente eficiente en aplicaciones donde se requiere realizar inferencias repetidas, como en control o diseño inverso (Kumar et al. 2024).",
    "crumbs": [
      "Redes neuronales",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>DeepONet</span>"
    ]
  },
  {
    "objectID": "PBHE.html#experimento",
    "href": "PBHE.html#experimento",
    "title": "Ecuación del Bio-Calor",
    "section": "Experimento",
    "text": "Experimento\nDurante su estudio, Pennes diseñó un experimento riguroso para medir la temperatura interna del antebrazo humano. Utilizó termopares tipo “Y” insertados transversalmente en la musculatura del antebrazo mediante una aguja estéril, como se ilustra en la Figura 1. Esta configuración permitía capturar un perfil térmico a lo largo del eje transversal, minimizando interferencias derivadas del contacto externo o la conducción axial no deseada.\nLa técnica experimental buscó máxima precisión geométrica y térmica: los termopares eran fijados con tensión controlada mediante un sistema mecánico que aseguraba trayectorias rectas y repetibles dentro del tejido. La inserción se realizaba con anestesia tópica mínima y bajo condiciones ambientales estables, lo cual garantizaba que los gradientes de temperatura registrados fueran atribuibles principalmente al metabolismo local y al efecto del flujo sanguíneo arterial.\n\n\n\n\n\n\nFigura 1: a) Posición del brazo derecho (vista superior). La linea horizontal II indica el nivel de la figura c). b) Posición del brazo derecho (vista lateral). c)Sección transversal anatómica del antebrazo en el nivel II (Pennes 1948).",
    "crumbs": [
      "Ecuación del Bio-Calor"
    ]
  },
  {
    "objectID": "PBHE.html#trascendencia",
    "href": "PBHE.html#trascendencia",
    "title": "Ecuación del Bio-Calor",
    "section": "Trascendencia",
    "text": "Trascendencia\nEl modelo de Pennes simplificó la complejidad biológica al asumir un flujo sanguíneo uniforme y una transferencia de calor proporcional a la diferencia entre la temperatura arterial y la tisular. Aunque posteriores investigaciones refinaron sus supuestos, su ecuación sigue siendo un referente en bioingeniería térmica. Su trabajo no solo sentó las bases para aplicaciones clínicas, como la hipertermia oncológica, sino que también inspiró avances en el estudio de la termorregulación humana y el diseño de dispositivos médicos.",
    "crumbs": [
      "Ecuación del Bio-Calor"
    ]
  },
  {
    "objectID": "ecuacion.html#versión-reducida-adimensionalizada",
    "href": "ecuacion.html#versión-reducida-adimensionalizada",
    "title": "8  Forma de la ecuación",
    "section": "8.1 Versión reducida (adimensionalizada)",
    "text": "8.1 Versión reducida (adimensionalizada)\nMediante escalamiento: \\[\\begin{equation*}\nT' = T - T_a \\qquad \\theta = \\dfrac{T'}{T_M - T_a} \\qquad X = \\dfrac{x}{L_0} \\qquad \\tau = \\dfrac{t}{t_f}\n\\end{equation*}\\]\n\n\n\nTabla 8.2: Tabla de nomenclatura de las relaciones para escalamiento.\n\n\n\n\n\nSímbolo\nDescripción\nUnidades\n\n\n\n\n\\(L_0\\)\nLongitud característica del dominio\n\\(m\\)\n\n\n\\(t_f\\)\nTiempo final de simulación\n\\(s\\)\n\n\n\n\n\n\nla Ecuación 8.1 se convierte en:\n\\[\n\\partial_{\\tau} \\theta = a_1 \\partial_{XX} \\theta - a_2 W \\theta + a_3\n\\tag{8.2}\\]\nParámetros adimensionales:\n- \\(a_1 = \\frac{t_f}{\\alpha L_0^2}\\) (difusividad térmica \\(\\alpha = \\frac{k_{\\text{eff}}}{\\rho c}\\)).\n- \\(a_2 = \\frac{t_f c_b}{\\rho c}\\).\n- \\(a_3 = \\frac{t_f Q}{\\rho c (T_M - T_a)}\\).\n- \\(W = \\rho_b \\omega_b\\): Tasa volumétrica de perfusión (kg/m³·s).",
    "crumbs": [
      "Ecuación del Bio-Calor",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Forma de la ecuación</span>"
    ]
  },
  {
    "objectID": "ecuacion.html#condiciones-de-uso-adecuadas",
    "href": "ecuacion.html#condiciones-de-uso-adecuadas",
    "title": "8  Forma de la ecuación",
    "section": "8.2 Condiciones de uso adecuadas",
    "text": "8.2 Condiciones de uso adecuadas\n\nTejidos homogéneos: Aproximación válida para regiones con propiedades térmicas uniformes.\n\nPerfusión sanguínea constante: Supone flujo sanguíneo estable en el dominio.\n\nAplicaciones clínicas: Hipertermia, crioterapia y modelado térmico en terapias oncológicas.",
    "crumbs": [
      "Ecuación del Bio-Calor",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Forma de la ecuación</span>"
    ]
  },
  {
    "objectID": "ejemplos.html#aplicaciones-recientes-de-la-ecuación-del-bio-calor",
    "href": "ejemplos.html#aplicaciones-recientes-de-la-ecuación-del-bio-calor",
    "title": "9  Modelado del Bio-Calor en Hipertermia",
    "section": "9.1 Aplicaciones recientes de la ecuación del bio-calor",
    "text": "9.1 Aplicaciones recientes de la ecuación del bio-calor\nQuintero et al. (2017) desarrollan un modelo basado en ecuaciones diferenciales parciales que integra la ecuación del bio-calor y la ley de Arrhenius para estimar el daño térmico en tratamientos de hipertermia superficial. Utilizan el método de líneas para resolver el sistema y plantean un problema de optimización que busca maximizar el daño al tejido tumoral minimizando el daño colateral. Su trabajo demuestra cómo la modelación matemática puede guiar estrategias terapéuticas más seguras y eficaces.\nDutta y Rangarajan (2018) presentan una solución analítica cerrada en dos dimensiones para la ecuación del bio-calor, considerando modelos de conducción tanto de tipo Fourier como no-Fourier. Mediante el uso de la transformada de Laplace, analizan la influencia de parámetros fisiológicos como la perfusión sanguínea y el tiempo de relajación térmica sobre la evolución de la temperatura. Su investigación aporta una base teórica sólida para comprender la propagación térmica en tejidos vivos durante la hipertermia terapéutica.\nYang et al. (2014) propone una estrategia numérica para resolver problemas inversos de conducción térmica en tejidos biológicos multicapa, utilizando un enfoque en diferencias finitas y el concepto de tiempo futuro. El estudio se enfoca en predecir las condiciones de frontera necesarias para generar distribuciones de temperatura deseadas. La implementación de este método permite estimar parámetros relevantes en tiempo real, lo cual resulta esencial para el control térmico preciso en procedimientos médicos no invasivos como la hipertermia localizada.",
    "crumbs": [
      "Ecuación del Bio-Calor",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Modelado del Bio-Calor en Hipertermia</span>"
    ]
  },
  {
    "objectID": "estudio_caso.html",
    "href": "estudio_caso.html",
    "title": "Estudio de caso",
    "section": "",
    "text": "Hipertermia como opción terapéutica complementaria en el manejo de cáncer",
    "crumbs": [
      "Estudio de caso"
    ]
  },
  {
    "objectID": "estudio_caso.html#hipertermia-como-opción-terapéutica-complementaria-en-el-manejo-de-cáncer",
    "href": "estudio_caso.html#hipertermia-como-opción-terapéutica-complementaria-en-el-manejo-de-cáncer",
    "title": "Estudio de caso",
    "section": "",
    "text": "La Organización Mundial de la Salud (2022) en su página web define Cáncer como:\n\n«Cáncer» es un término genérico utilizado para designar un amplio grupo de enfermedades que pueden afectar a cualquier parte del organismo; también se habla de «tumores malignos» o «neoplasias malignas». Una característica definitoria del cáncer es la multiplicación rápida de células anormales que se extienden más allá de sus límites habituales y pueden invadir partes adyacentes del cuerpo o propagarse a otros órganos, en un proceso que se denomina «metástasis». La extensión de las metástasis es la principal causa de muerte por la enfermedad.\n\nPor su parte Instituto Nacional del Cáncer (2021) aporta lo siguiente:\n\nEs posible que el cáncer comience en cualquier parte del cuerpo humano, formado por billones de células. En condiciones normales, las células humanas se forman y se multiplican (mediante un proceso que se llama división celular) para formar células nuevas a medida que el cuerpo las necesita. Cuando las células envejecen o se dañan, mueren y las células nuevas las reemplazan. A veces el proceso no sigue este orden y las células anormales o células dañadas se forman y se multiplican cuando no deberían. Estas células tal vez formen tumores, que son bultos de tejido. Los tumores son cancerosos (malignos) o no cancerosos (benignos).\n\n\n\n\n\n\n\nFigura 1: Una célula de cáncer de seno que se multiplica (Instituto Nacional del Cáncer 2021).\n\n\n\nÉsta enfermedad es la principal causa de muerte a nivel mundial, solo en 2020 arrebató casi 10 millones de vidas y según datos de Organización Mundial de la Salud (2022) los cánceres más comunes en 2020 fueron:\n\nDe mama (2.26 millones de casos)\nDe pulmón (2.21 millones de casos)\nDe colon (1.93 millones de casos)\nDe próstata (1.41 millones de casos)\nDe piel (distinto del melanoma) (1.20 millones de casos)\nGástrico (1.09 millones de casos)\n\nEs ante este panorama que distintos tratamientos surgen con el objetivo de erradicar la enfermedad siempre que se tenga una detección oportuna. Uno de dichos tratamientos es la hipertermia, según en el National Cancer Institute (2021) es un método que consiste en calentar el tejido corporal hasta los 39-45 °C para ayudar a erradicar células cancerígenas con pequeñas o nulas lesiones en el tejido sano. La hipertermia también es llamada terapia térmica o termoterapia.\nUno de los principales retos de este tratamiento es la creación de un modelo óptimo que se adecue al comportamiento de la transferencia de calor que se hace a los tejidos con el fin de dañar únicamente el área en el que se encuentran las célular cancerígenas, es por ello que los modelos de integencia artificial y más precisamente las PINN’s (aqui irá una cita) surgen como posible solución a este reto.\nEl presente estudio utilizó como punto de partida el trabajo realizado por Alessio Borgi (2023) para modelar el calentamiento del tejido corporal usando la ecuación del Bio-Calor en dos dimensiones.\n\n\n\n\n\nAlessio Borgi, Alessandro De Luca, Eugenio Bugli. 2023. «BioHeat PINNs: Temperature Estimation with Bio-Heat Equation using Physics-Informed Neural Networks». https://github.com/alessioborgi/BioHeat_PINNs/tree/main?tab=readme-ov-file#bioheat-pinns-temperature-estimation-with-bio-heat-equation-using-physics-informed-neural-networks.\n\n\nInstituto Nacional del Cáncer. 2021. «¿Qué es el cáncer?» https://www.cancer.gov/espanol/cancer/naturaleza/que-es.\n\n\nNational Cancer Institute. 2021. «Hyperthermia to Treat Cancer». https://www.cancer.gov/about-cancer/treatment/types/hyperthermia.\n\n\nOrganización Mundial de la Salud. 2022. «Cáncer». https://www.who.int/es/news-room/fact-sheets/detail/cancer.",
    "crumbs": [
      "Estudio de caso"
    ]
  },
  {
    "objectID": "metodologia.html#aportaciones-del-modelo",
    "href": "metodologia.html#aportaciones-del-modelo",
    "title": "10  Metodología",
    "section": "10.1 Aportaciones del modelo",
    "text": "10.1 Aportaciones del modelo\nYa que se parte del trabajo de Alessio Borgi (2023), se examinó que dos de los puntos a mejorar de la red neuronal que plantearon son:\n\nDesarrollar nuevas arquitecturas para la red neuronal y explorar nuevas configuraciones\nCombinar las fortalezas de los algoritmos de optimización Adam y L-BFGS para mejorar la velocidad de convergencia y la precisión\n\nTenindo los anteriores puntos en cuenta, se procedió a abordarlos e implementarlos dentro del diseño del modelo.",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodología</span>"
    ]
  },
  {
    "objectID": "metodologia.html#diseño-del-modelo",
    "href": "metodologia.html#diseño-del-modelo",
    "title": "10  Metodología",
    "section": "10.2 Diseño del modelo",
    "text": "10.2 Diseño del modelo\nEl lenguaje seleccionado fué Python, a su vez el código se basa enteramente en la librería Deepxde creada por Lu et al. (2021) la cual está directamente enfocada a resolver ecuaciones diferenciales, se usó además como backend tensorflow_compat_v1 siendo su elección debida únicamente a la familiarización previa que se tenía con ella. Finalmente el entorno donde se programó y optimizó el código fué en Google Colab ya que la potencia de cómputo ofrecida por la plataforma era necesaria para ejecutar el modelo.",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodología</span>"
    ]
  },
  {
    "objectID": "metodologia.html#implementación-del-modelo",
    "href": "metodologia.html#implementación-del-modelo",
    "title": "10  Metodología",
    "section": "10.3 Implementación del modelo",
    "text": "10.3 Implementación del modelo\nLa implementación del modelo se llevó a cabo en dos etapas clave: (1) el desarrollo del código base para resolver la ecuación del Bio-Calor mediante DeepXDE, y (2) la optimización sistemática de los hiperparámetros. Para esta última, se siguieron las recomendaciones del estudio de Alessio Borgi (2023), adaptadas a las particularidades del problema. Se ajustaron parámetros críticos como el número de épocas de entrenamiento (iterations), el ratio de aprendizaje (learning rate) así como un decaimiento en el mismo dependiente de la itearación actual (decay), la función de activación (elu) y el esquema de inicialización de pesos (Glorot normal). Estos ajustes se realizaron mediante un proceso iterativo que buscaba minimizar la función de pérdida mientras se mantenía un tiempo de entrenamiento computacionalmente viable.",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodología</span>"
    ]
  },
  {
    "objectID": "metodologia.html#evaluación-del-modelo",
    "href": "metodologia.html#evaluación-del-modelo",
    "title": "10  Metodología",
    "section": "10.4 Evaluación del modelo",
    "text": "10.4 Evaluación del modelo\nPara validar el desempeño del modelo propuesto, se realizó una evaluación exhaustiva utilizando un conjunto de datos independiente, el cual no fue empleado durante las fases de entrenamiento o ajuste de hiperparámetros. Este enfoque garantiza una medición objetiva de la capacidad de generalización del modelo ante datos no vistos.\nLas predicciones generadas por el modelo fueron analizadas mediante visualizaciones espaciotemporales, las cuales permiten comparar cualitativamente el comportamiento de las soluciones pronosticadas frente a los rangos físicos y temporales definidos en el problema. En particular, se generaron gráficas de superficies 3D que muestran la evolución de las variables de interés a lo largo del dominio espacial y temporal bajo estudio. Adicionalmente, se incluyeron representaciones de cortes transversales y series temporales en puntos estratégicos para facilitar la interpretación de los resultados.\nCabe destacar que este análisis preliminar se centró en examinar la coherencia física y la estabilidad numérica de las predicciones. Para la evaluación cuantitativa del modelo, se implementó una comparación directa con las soluciones obtenidas mediante el método numérico de Crank-Nicolson, resuelto en Julia utilizando la librería DifferentialEquations.jl.",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodología</span>"
    ]
  },
  {
    "objectID": "metodologia.html#comparación-de-resultados",
    "href": "metodologia.html#comparación-de-resultados",
    "title": "10  Metodología",
    "section": "10.5 Comparación de resultados",
    "text": "10.5 Comparación de resultados\nPara evaluar el desempeño predictivo del modelo propuesto, se realizaron dos tipos de comparaciones:\n\nUna evaluación cualitativa basada en visualizaciones.\nUn análisis cuantitativo mediante métricas de error estandarizadas.\n\nEn primer lugar, se llevó a cabo una comparación visual con los resultados reportados en el trabajo de Alessio Borgi (2023), dado que dicho estudio no incluye datos numéricos tabulados, sino únicamente representaciones gráficas de las soluciones. Esta comparación permitió identificar coincidencias y discrepancias en el comportamiento espaciotemporal de las variables de interés, destacando las fortalezas del modelo propuesto en términos de estabilidad numérica.\nEn segundo lugar, para una evaluación cuantitativa rigurosa, se compararon las predicciones del modelo con soluciones de referencia generadas mediante el método de Crank-Nicolson, implementado en Julia utilizando la librería DifferentialEquations.jl. La comparación se realizó sobre una malla uniforme de 26×26 puntos en el cuadrado de \\([0,1]\\times[0,1]\\), calculando para cada instante de tiempo relevante las siguientes métricas:\n\nError Absoluto Medio (MAE)\nError Absoluto Máximo (MaxAE)\nError L2 (norma euclidiana)\n\nEstos criterios permitieron cuantificar no solo la precisión global del modelo, sino también sus desviaciones locales más significativas, particularmente en regiones con alta variabilidad espacial. Los resultados detallados de este análisis, junto con una discusión sobre la eficiencia computacional relativa entre ambos métodos, se presentan en la Tabla 13.1.",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodología</span>"
    ]
  },
  {
    "objectID": "metodologia.html#análisis-y-conclusión",
    "href": "metodologia.html#análisis-y-conclusión",
    "title": "10  Metodología",
    "section": "10.6 Análisis y conclusión",
    "text": "10.6 Análisis y conclusión\nFinalmente, se realizó un análisis detallado de los resultados obtenidos para extraer conclusiones significativas. Se proporcionaron recomendaciones basadas en los hallazgos del estudio, lo que permitió establecer un marco para interpretaciones analíticas profundas y recomendaciones bien fundamentadas en la sección de conclusiones del estudio.\nEste enfoque metodológico proporcionó una base sólida para los resultados obtenidos, asegurando la integridad y la calidad del análisis realizado en el estudio.",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Metodología</span>"
    ]
  },
  {
    "objectID": "pred_crank_nick.html#guardado-de-datos",
    "href": "pred_crank_nick.html#guardado-de-datos",
    "title": "11  Predicciones del método numérico",
    "section": "11.1 Guardado de datos",
    "text": "11.1 Guardado de datos\nCon el objetivo de optimizar el proceso de comparación cuantitativa con el modelo de redes neuronales, se exportó un subconjunto representativo de los resultados. Aunque la simulación original utilizó una malla de 51×51 puntos, se almacenaron únicamente los valores correspondientes a una grilla de 26×26 puntos. Esta decisión se basó en:\n\nSuficiencia estadística: La densidad de puntos conserva los patrones espaciales críticos.\nEficiencia computacional: Reduce el tamaño del archivo sin perder información relevante.\n\nLos datos se guardaron en un archivo CSV estructurado con las siguientes columnas:\n\nCoordenadas espacio-temporales (t, x, y) para cada punto de la grilla 26×26.\nValores de la solución en los tiempos de interés.\n\nEste archivo permitió calcular de manera estandarizada las métricas de error (MAE, MaxAE, error L2) en la sección de comparación de resultados.",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Predicciones del método numérico</span>"
    ]
  },
  {
    "objectID": "model_don.html#gráficas-de-pérdida-del-modelo",
    "href": "model_don.html#gráficas-de-pérdida-del-modelo",
    "title": "12  Métricas del modelo",
    "section": "12.1 Gráficas de pérdida del modelo",
    "text": "12.1 Gráficas de pérdida del modelo\nEl proceso de entrenamiento del modelo se monitoreó mediante el seguimiento detallado de cinco componentes de pérdida, cada una asociada a restricciones físicas y matemáticas específicas del problema:\n\nPérdida residual de la EDP\n\nFunción: Mide el cumplimiento de la ecuación de Bio-Calor en el dominio interior.\nImportancia: Garantiza que la solución aprendida satisfaga la física subyacente.\nComportamiento esperado: Debe converger a valores cercanos a cero (típicamente &lt; 1e-3).\n\nPérdida de condición inicial\n\nFunción: Controla la precisión en t=0.\nImportancia: Asegura coherencia con el estado inicial del sistema.\nPatrón típico: Suele ser la primera en converger por su carácter puntual.\n\nPérdida de frontera izquierda (Dirichlet)\n\nFunción: Evalúa el cumplimiento de condiciones de valor prescrito.\nRelevancia: Mantiene valores fijos en bordes específicos.\nConvergencia: Normalmente rápida por ser restrictiva.\n\nPérdida de frontera derecha (Neumann)\n\nFunción: Verifica gradientes normales en esta frontera\nDificultad característica: Puede mostrar oscilaciones iniciales\n\nPérdida de fronteras superior/inferior (Neumann)\n\nFunción: Controla condiciones de flujo en estos bordes\nComplejidad: En problemas 2D/3D suele ser la última en estabilizarse\n\n\n\n12.1.1 Perdida para el conjunto de entrenamiento\n\n\nCódigo\nimport plotly.graph_objects as go\n\n# Nombres de las componentes del loss\nloss_labels = [\n    \"PDE residual loss\",\n    \"Initial‐condition loss\",\n    \"Left‐boundary (Dirichlet) loss\",\n    \"Right‐boundary (Neumann) loss\",\n    \"Top/Bottom‐boundary (Neumann) loss\"\n]\n\n# Extraer pasos y pérdida de entrenamiento\nsteps = losshistory.steps\ntrain_loss = np.array(losshistory.loss_train)\n\n# Crear figura\nfig_train = go.Figure()\n\nfor i in range(train_loss.shape[1]):\n    fig_train.add_trace(go.Scatter(\n        x=steps,\n        y=train_loss[:, i],\n        mode='lines',\n        name=loss_labels[i]\n    ))\n\nfig_train.update_layout(\n    title=\"Training Loss history\",\n    xaxis=dict(title=\"Iteration\", tickformat=\".1e\"),\n    yaxis=dict(title=\"Loss\", type=\"log\", tickformat=\".1e\"),\n    template=\"plotly_white\",\n    legend=dict(x=0.99, y=0.99),\n    font=dict(size=14)\n)\n\n\n\n\n\n\n                                                \n\n\nFigura 12.1: Historial de pérdida en el entrenamiento de la red neuronal.\n\n\n\n\n\n\n12.1.2 Pérdida para el conjunto de prueba\n\n\nCódigo\nimport plotly.graph_objects as go\n\n# Nombres de las componentes del loss\nloss_labels = [\n    \"PDE residual loss\",\n    \"Initial‐condition loss\",\n    \"Left‐boundary (Dirichlet) loss\",\n    \"Right‐boundary (Neumann) loss\",\n    \"Top/Bottom‐boundary (Neumann) loss\"\n]\n\n# Extraer pasos y pérdida de entrenamiento\nsteps = losshistory.steps\ntest_loss = np.array(losshistory.loss_test)\n\n# Crear figura\nfig_test = go.Figure()\n\nfor i in range(test_loss.shape[1]):\n    fig_test.add_trace(go.Scatter(\n        x=steps,\n        y=test_loss[:, i],\n        mode='lines',\n        name=loss_labels[i]\n    ))\n\nfig_test.update_layout(\n    title=\"Test Loss history\",\n    xaxis=dict(title=\"Iteration\", tickformat=\".1e\"),\n    yaxis=dict(title=\"Loss\", type=\"log\", tickformat=\".1e\"),\n    template=\"plotly_white\",\n    legend=dict(x=0.99, y=0.99),\n    font=dict(size=14)\n)\n\n\n\n\n\n\n                                                \n\n\nFigura 12.2: Historial de pérdida en el conjunto de prueba de la red neuronal.",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Métricas del modelo</span>"
    ]
  },
  {
    "objectID": "model_don.html#guardado-de-datos",
    "href": "model_don.html#guardado-de-datos",
    "title": "12  Métricas del modelo",
    "section": "12.2 Guardado de datos",
    "text": "12.2 Guardado de datos\nPara permitir la comparación cuantitativa con el método de Crank-Nicolson y facilitar la generación de visualizaciones consistentes, se exportaron las predicciones del modelo neuronal en formato CSV. El proceso consistió en:\n\nGeneración de la malla de evaluación:\n\nDominio espacial: Cuadrado unitario [0,1] × [0,1].\nDiscretización: 26 segmentos equiespaciados en cada eje (x, y).\nPuntos totales: 676 (26 × 26)\nTiempos evaluados: t = [0.0, 0.25, 0.50, 0.75, 1.0].\n\nEstructura del archivo:\n\nCoordenadas espacio-temporales (t, x, y) para cada punto de la grilla 26×26.\nValores de la solución en los tiempos de interés.\n\n\n\n\nCódigo\nimport pandas as pd\n# Lista de tiempos\ntimes = [0.0, 0.25, 0.5, 0.75, 1.0]\n\n# Crear la malla (x, y)\nnum_points = 26\nx = np.linspace(0, 1, num_points)\ny = np.linspace(0, 1, num_points)\nX, Y = np.meshgrid(x, y)\n\n# Lista para almacenar resultados\nresults = []\n\nfor t_val in times:\n    # Crear entrada trunk: (num_points^2, 3)\n    points = np.vstack((X.flatten(), Y.flatten(), t_val * np.ones_like(X.flatten()))).T\n\n    # Crear entrada branch: condición inicial constante cero\n    branch_input = np.zeros((1, sensor_pts.shape[0]))\n\n    # Predecir\n    predicted = model.predict((branch_input, points)).flatten()\n\n    # Agregar los datos al resultado\n    for xi, yi, thetai in zip(points[:, 0], points[:, 1], predicted):\n        results.append([t_val, xi, yi, thetai])\n\n# Crear el DataFrame\ndf = pd.DataFrame(results, columns=[\"time\", \"X\", \"Y\", \"Theta\"])\n\n# Obtener la ruta del script actual y guardar el archivo CSV\nruta = r\"data/model_DoN.csv\"\ndf.to_csv(ruta, index=False)\n\n\n\n12.2.1 Visualización interactiva\nPara facilitar el análisis exploratorio de las predicciones generadas por el modelo DeepONet, se implementó una tabla dinámica interactiva mediante la librería itables (versión 1.5.2), que extiende las funcionalidades de Pandas para su visualización.\n\n\nCódigo\nfrom itables import show, options\n\noptions.maxBytes = 0\n\n# Mostrar la tabla interactiva\nshow(df, paging=True,\n        ordering=True,\n        searching = False,\n        scrollY=\"350px\",\n        buttons=[\"pageLength\", \"csv\", \"excel\"],\n        lengthMenu=[10, 25, 50, 100],\n        classes=\"display nowrap cell-border\"\n        )\n\n\n\n\nTabla 12.1: Predicciones de la red neuronal.\n\n\n\n\n\n    \n      \n      time\n      X\n      Y\n      Theta\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Métricas del modelo</span>"
    ]
  },
  {
    "objectID": "comparaciones.html#comparativa-visual-de-las-predicciones",
    "href": "comparaciones.html#comparativa-visual-de-las-predicciones",
    "title": "13  Comparación de resultados",
    "section": "13.1 Comparativa visual de las predicciones",
    "text": "13.1 Comparativa visual de las predicciones\nEsta sección presenta un análisis cualitativo de los resultados mediante la comparación directa entre las predicciones del modelo, las soluciones reportadas en el estudio de Alessio Borgi (2023) y las obtenidas mediante el método de Crank Nickolson. La visualización paralela permite evaluar:\n\nDominio espacial: Cuadrado unitario [0,1] × [0,1] con malla 26×26.\nEscala de colores: Mapa térmico viridis (consistente en ambas columnas).\n\n\n13.1.1 Modelo contra resultados de Alessio Borgi (2023)\n\n\nCódigo\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.gridspec as gridspec\nimport pandas as pd\nimport numpy as np\n\n# Lista de tiempos\ntimes = [0.0, 0.25, 0.5, 0.75, 1.0]\n\n# Cargar el dataframe\ndf = pd.read_csv(r'data/model_DoN.csv')  \n\n# Crear figura con subplots 3D en 1 fila y 5 columnas\nfig, axes = plt.subplots(nrows=1, ncols=len(times), \n                        figsize=(22, 6),\n                        subplot_kw={'projection': '3d'})\n\n# Asumimos que el grid es regular\nnum_points = int(np.sqrt(df[df[\"time\"] == times[0]].shape[0]))\n\n# Lista para almacenar los objetos surface\nsurf_list = []\n\n# Reordenar para graficar\nfor i, (t_val, ax) in enumerate(zip(times, axes)):\n    # Filtrar por tiempo actual\n    df_t = df[df[\"time\"] == t_val]\n\n    # Obtener los valores de X, Y, Theta\n    X_vals = df_t[\"X\"].values.reshape((num_points, num_points))\n    Y_vals = df_t[\"Y\"].values.reshape((num_points, num_points))\n    Z_vals = df_t[\"Theta\"].values.reshape((num_points, num_points))\n\n    # Dibujar la superficie\n    surf = ax.plot_surface(\n        Y_vals, X_vals, Z_vals,\n        rstride=1, cstride=1,\n        cmap=\"YlGnBu\",\n        edgecolor=\"none\",\n        antialiased=True\n    )\n    surf_list.append(surf)\n\n    ax.set_title(f\"Time = {t_val:.2f} s\", pad=10)\n    ax.set_xlabel(\"Y\", labelpad=10)\n    ax.set_ylabel(\"X\", labelpad=10)\n    ax.set_zlabel(\"T [K]\", labelpad=10, rotation=90)\n\n\n# Añadir barra de color común\ncbar = fig.colorbar(surf_list[-1], ax=axes, shrink=0.9, aspect=90, pad=0.1, orientation='horizontal')\ncbar.set_label('Temperatura [K]')\n\nplt.show()\n\n\n\n\n\n\n\n\nFigura 13.1: Predicciones de la red neuronal a distintos tiempos.\n\n\n\n\n\n\n\n\n\n\n\nFigura 13.2: Resultados reportados por Alessio Borgi (2023) en el caso 2D.\n\n\n\n\n\n13.1.2 Modelo contra método numérico\n\n\nCódigo\ncrank_nick_data = pd.read_csv(r'data/crank_nick.csv')\nmodel_don_data = pd.read_csv(r'data/model_DoN.csv')\n\n# Determinar los límites comunes para el colorbar\nmin_temp = min(model_don_data['Theta'].min(), crank_nick_data['Theta'].min())\nmax_temp = max(model_don_data['Theta'].max(), crank_nick_data['Theta'].max())\n\n# Crear figura con subplots 3D en 2 filas y 5 columnas\nfig = plt.figure(figsize=(22, 12))\naxes = []\n\n# Crear los subplots\nfor i in range(2):  # 2 filas\n    for j in range(5):  # 5 columnas\n        axes.append(fig.add_subplot(2, 5, i*5 + j + 1, projection='3d'))\n\naxes = np.array(axes).reshape(2, 5)  # Convertir a matriz 2x5 para acceder fácilmente\n\n# Añadir títulos generales para cada fila\nfig.text(0.5, 0.92, \"Predicciones modelo DON\", ha='center', va='center', fontsize=14, fontweight='bold')\nfig.text(0.5, 0.56, \"Predicciones método numérico\", ha='center', va='center', fontsize=14, fontweight='bold')\n\n# Función para graficar un dataframe en una fila específica\ndef plot_dataframe(df, row, num_points, cmap=\"viridis\"):\n    surf_list = []\n    for col, t_val in enumerate(times):\n        ax = axes[row, col]\n        \n        # Filtrar por tiempo actual\n        df_t = df[df[\"time\"] == t_val]\n\n        # Obtener los valores de X, Y, Theta\n        X_vals = df_t[\"X\"].values.reshape((num_points, num_points))\n        Y_vals = df_t[\"Y\"].values.reshape((num_points, num_points))\n        Z_vals = df_t[\"Theta\"].values.reshape((num_points, num_points))\n\n        # Dibujar la superficie con límites comunes\n        surf = ax.plot_surface(\n            Y_vals, X_vals, Z_vals,\n            rstride=1, cstride=1,\n            cmap=cmap,\n            edgecolor=\"none\",\n            antialiased=True,\n            vmin=min_temp,\n            vmax=max_temp\n        )\n        surf_list.append(surf)\n\n        ax.set_title(f\"Time = {t_val:.2f} s\", pad=10)\n        ax.set_xlabel(\"Y\", labelpad=10)\n        ax.set_ylabel(\"X\", labelpad=10)\n        ax.set_zlabel(\"T [K]\", labelpad=10, rotation=90)\n    \n    return surf_list\n\n# Asumimos que el grid es regular para ambos dataframes\nnum_points = int(np.sqrt(model_don_data[model_don_data[\"time\"] == times[0]].shape[0]))\n\n# Graficar el primer dataframe en la fila superior\nsurf_model_don = plot_dataframe(model_don_data, 0, num_points)\n\n# Graficar el segundo dataframe en la fila inferior\nsurf_crank_nick = plot_dataframe(crank_nick_data, 1, num_points)\n\n# Añadir barra de color común en la parte inferior\ncbar = fig.colorbar(surf_crank_nick[-1], ax=axes.ravel().tolist(),\n                    use_gridspec=True, orientation='horizontal',\n                    pad=0.05, aspect=90, shrink=0.9)\ncbar.set_label('Temperatura [K]', labelpad=10)\n\n\nplt.show()\n\n\n\n\n\n\n\n\nFigura 13.3: Contraste de las predicciones entre el método de Crank Nikolson y el modelo para cada tiempo.",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Comparación de resultados</span>"
    ]
  },
  {
    "objectID": "comparaciones.html#validación-cuantitativa-frente-al-método-de-crank-nicolson",
    "href": "comparaciones.html#validación-cuantitativa-frente-al-método-de-crank-nicolson",
    "title": "13  Comparación de resultados",
    "section": "13.2 Validación Cuantitativa frente al Método de Crank-Nicolson",
    "text": "13.2 Validación Cuantitativa frente al Método de Crank-Nicolson\nPara evaluar numéricamente la precisión del modelo DeepONet, se realizó una comparación sistemática con soluciones de referencia generadas mediante el método de Crank-Nicolson. Este enfoque proporciona una métrica objetiva de la exactitud del modelo, siendo complementado con una serie de gráficos que muestran el error absoluto para cada punto del dominio en los tiempos de interés.\n\n\nCódigo\n# Función para calcular errores\ndef calculate_errors(true_data, pred_data, times):\n    results = []\n    \n    for time in times:\n        # Filtrar datos por tiempo\n        true_subset = true_data[true_data['time'] == time]\n        pred_subset = pred_data[pred_data['time'] == time]\n        \n        if len(true_subset) == 0 or len(pred_subset) == 0:\n            print(f\"Advertencia: No hay datos para tiempo t={time}\")\n            continue\n        \n        # Verificar que las dimensiones coincidan\n        if len(true_subset) != len(pred_subset):\n            print(f\"Advertencia: Número de puntos no coincide para t={time}\")\n            min_len = min(len(true_subset), len(pred_subset))\n            true_subset = true_subset.iloc[:min_len]\n            pred_subset = pred_subset.iloc[:min_len]\n        \n        # Calcular errores para Theta\n        theta_true = true_subset['Theta'].values\n        theta_pred = pred_subset['Theta'].values\n        \n        absolute_error = np.abs(theta_true - theta_pred)\n        l2_error = np.sqrt(np.sum((theta_true - theta_pred)**2))\n        \n        results.append({\n            'time': time,\n            'mean_absolute_error': np.mean(absolute_error),\n            'max_absolute_error': np.max(absolute_error),\n            'l2_error': l2_error\n        })\n    \n    return pd.DataFrame(results)\n\n# Calcular errores\nerror_results = calculate_errors(crank_nick_data, model_don_data, times)\n\n# Guardar resultados\nerror_results.to_csv(\"data/error_comparison.csv\", index=False)\n\n\n\n\n\nTabla 13.1: Error del modelo DeepONet respecto a Crank-Nicolson.\n\n\n\n|   Tiempo |   MAE |   MaxAE |   Error L2 |\n|---------:|------:|--------:|-----------:|\n|    0.000 | 0.017 |   0.064 |      0.560 |\n|    0.250 | 0.068 |   0.172 |      2.202 |\n|    0.500 | 0.053 |   0.102 |      1.581 |\n|    0.750 | 0.011 |   0.039 |      0.343 |\n|    1.000 | 0.067 |   0.200 |      2.305 |\n\n\n\n\n\n13.2.1 Gráficas de error absoluto\n\n\nCódigo\n# Calcular el error absoluto entre los dos dataframes\nerror_data = model_don_data.copy()\nerror_data['error'] = np.abs(crank_nick_data['Theta'] - model_don_data['Theta'])\n\n# Crear figura con 3 filas y 2 columnas\nfig, axes = plt.subplots(nrows=1, ncols=5, figsize=(22, 6))\naxes = axes.ravel()  # Convertir a array 1D para fácil acceso\n\n# Asumir que el grid es regular\nnum_points = int(np.sqrt(error_data[error_data[\"time\"] == times[0]].shape[0]))\n\n# Configuración común para los mapas de calor\nplot_kwargs = {\n    'cmap': 'hot_r',\n    'shading': 'auto',\n    'vmin': error_data['error'].min(),\n    'vmax': error_data['error'].max()\n}\n# Lista para guardar los gráficos\nabs_errors_pc = []\n\n# Crear los subplots\nfor i, t_val in enumerate(times):\n    ax = axes[i]\n    \n    # Filtrar por tiempo actual\n    df_t = error_data[error_data[\"time\"] == t_val]\n    \n    # Obtener valores y reshape\n    X_vals = df_t[\"X\"].values.reshape((num_points, num_points))\n    Y_vals = df_t[\"Y\"].values.reshape((num_points, num_points))\n    error_vals = df_t[\"error\"].values.reshape((num_points, num_points))\n    \n    # Crear mapa de calor\n    pc = ax.pcolormesh(X_vals, Y_vals, error_vals, **plot_kwargs)\n    \n    # Configuración de ejes\n    ax.set_title(f\"Tiempo = {t_val:.2f} s\", pad=10)\n    ax.set_xlabel(\"X\")\n    ax.set_ylabel(\"Y\")\n    ax.set_aspect('equal')\n\n    abs_errors_pc.append(pc)\n\ncbar = fig.colorbar(abs_errors_pc[-1], ax=axes, use_gridspec=True, shrink=0.9, aspect=90, pad=0.1, orientation='horizontal')\ncbar.set_label('Error absoluto [K]')\n\n# Mostrar el gráfico\nplt.show()\n\n\n\n\n\n\n\n\nFigura 13.4: Errores absolutos entre el método de Crank Nikolson y el modelo para cada tiempo.",
    "crumbs": [
      "Estudio de caso",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Comparación de resultados</span>"
    ]
  },
  {
    "objectID": "conclusiones.html",
    "href": "conclusiones.html",
    "title": "14  Conclusiones",
    "section": "",
    "text": "El presente trabajo ha abordado la complejidad de resolver una ecuación diferencial parcial dependiente del tiempo en dos dimensiones espaciales a través de una red neuronal con la arquitectura DeepONet, asimismo se obtuvieron predicciones para el cuadrado de \\([0,1]\\times[0,1]\\). Los hiperparámetros de la red se fueron variando para obtener la mejor configuración, usando como base los resultados obtenidos por Alessio Borgi (2023). Los resultados obtenidos mediante la comparación con el método de Crank Nickolson demostraron que la red neuronal se aproxima con un mínimo del 1.1% y un máximo del 6.7% para el error medio absoluto, y cometiendo un error mínimo del 6.4% y máximo del 20% para el error máximo absoluto.\nLos errores obtenidos demuestran la eficacia del modelo para converger a la condición inicial, pues tal como se aprecia en la Figura 13.4, a medida que la ecuación evoluciona en el tiempo, las predicciones entre el método numérico y la red neuronal divergen. Aunque esto es probable que se deba a la naturaleza del método de Crank Nickolson, pues un pequeño error al inicio ocasionaría que conforme evoluciona la función, ésta cada vez se aleje más del valor real. Desafortunadamente no se cuenta con una base de datos que sirva como conjunto de validación por lo que no se puede asegurar que los resultados del método numérico sean los más óptimos para tomar como referencia.\n\n\n\n\n\nAlessio Borgi, Alessandro De Luca, Eugenio Bugli. 2023. «BioHeat PINNs: Temperature Estimation with Bio-Heat Equation using Physics-Informed Neural Networks». https://github.com/alessioborgi/BioHeat_PINNs/tree/main?tab=readme-ov-file#bioheat-pinns-temperature-estimation-with-bio-heat-equation-using-physics-informed-neural-networks.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Conclusiones</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Alessio Borgi, Alessandro De Luca, Eugenio Bugli. 2023. “BioHeat PINNs: Temperature Estimation with Bio-Heat\nEquation using Physics-Informed Neural Networks.” https://github.com/alessioborgi/BioHeat_PINNs/tree/main?tab=readme-ov-file#bioheat-pinns-temperature-estimation-with-bio-heat-equation-using-physics-informed-neural-networks.\n\n\nBlechschmidt, Jan, and Oliver G. Ernst. 2021. “Three Ways to Solve\nPartial Differential Equations with Neural Networks—a Review.”\nGAMM-Mitteilungen 44 (2): e202100006. https://doi.org/10.1002/gamm.202100006.\n\n\nBurden, Richard L., and J. Douglas Faires. 2010. “Numerical\nAnalysis.” In Numerical Analysis, 9th ed., 259–64.\nBoston, USA: Brooks Cole.\n\n\nDutta, Abhijit, and Gopal Rangarajan. 2018. “Diffusion in\nPharmaceutical Systems: Modelling and Applications.” Journal\nof Pharmacy and Pharmacology 70 (5): 581–98. https://doi.org/10.1111/jphp.12885.\n\n\nInstituto Nacional del Cáncer. 2021. “¿Qué es\nel cáncer?” https://www.cancer.gov/espanol/cancer/naturaleza/que-es.\n\n\nKarniadakis, George Em, Ioannis G. Kevrekidis, Lu Lu, Paris Perdikaris,\nSifan Wang, and Liu Yang. 2021. “Physics-Informed Machine\nLearning.” Nature Reviews Physics 3 (6): 422–40. https://doi.org/10.1038/s42254-021-00314-5.\n\n\nKumar, Varun, Somdatta Goswami, Katiana Kontolati, Michael D. Shields,\nand George Em Karniadakis. 2024. “Synergistic Learning with\nMulti-Task DeepONet for Efficient PDE Problem Solving.” arXiv\nPreprint arXiv:2408.02198. https://arxiv.org/abs/2408.02198.\n\n\nLu, Lu, Pengzhan Jin, Guofei Pang, Zhongqiang Zhang, and George Em\nKarniadakis. 2021. “Learning Nonlinear Operators via DeepONet\nBased on the Universal Approximation Theorem of Operators.”\nNature Machine Intelligence 3 (3): 218–29. https://doi.org/10.1038/s42256-021-00302-5.\n\n\nLu, Lu, Xuhui Meng, Zhiping Mao, and George Em Karniadakis. 2021.\n“DeepXDE: A Deep Learning Library for Solving\nDifferential Equations.” SIAM Review 63 (1): 208–28. https://doi.org/10.1137/19M1274067.\n\n\nNational Cancer Institute. 2021. “Hyperthermia to Treat Cancer.” https://www.cancer.gov/about-cancer/treatment/types/hyperthermia.\n\n\nOrganización Mundial de la Salud. 2022.\n“Cáncer.” https://www.who.int/es/news-room/fact-sheets/detail/cancer.\n\n\nPennes, H. H. 1948. “Analysis of Tissue and Arterial Blood\nTemperatures in the Resting Human Forearm.” Journal of\nApplied Physiology 1 (2): 93–122. https://doi.org/10.1152/jappl.1948.1.2.93.\n\n\nQuintero, Luis A., Mauricio Peñuela, Armando Zambrano, and Edwin\nRodríguez. 2017. “Optimización Del Proceso de Preparación de\nSoluciones Madre de Antibióticos En Un Servicio Farmacéutico\nHospitalario.” Revista Cubana de Farmacia 50 (2):\n448–65. https://www.medigraphic.com/cgi-bin/new/resumen.cgi?IDARTICULO=75483.\n\n\nYang, Lihong, Xin Wu, Qian Wan, Jian Kong, Rui Liu, and Xiaoxi Liu.\n2014. “Pharmaceutical Preparation of Antibiotics: A Review on\nFormulation and Technique.” Asian Journal of Pharmaceutical\nSciences 9 (3): 145–53. https://doi.org/10.1016/j.ajps.2014.04.001.\n\n\nZill, Dennis G., and Michael R. Cullen. 2008. “Differential\nEquations with Boundary-Value Problems.” In, 7th ed., 433–42.\nBelmont, CA: Cengage Learning.",
    "crumbs": [
      "Referencias"
    ]
  }
]